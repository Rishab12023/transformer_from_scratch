{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83548c16",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Why Normalization Helps in Deep Learning\n",
    "\n",
    "In gradient-based learning models, maintaining **stable input distributions** is critical for efficient and reliable training. If the inputs to a layer are not well-scaled ‚Äî i.e., if their magnitudes are too large (exploding) or too small (vanishing) ‚Äî it can lead to **unstable gradients** during backpropagation. This causes weight updates to become erratic or ineffective, slowing down or even preventing convergence.\n",
    "\n",
    "During training, as data flows through layers and undergoes various transformations (especially non-linearities like ReLU, tanh, etc.), the distribution of activations can shift ‚Äî a phenomenon known as **internal covariate shift**. This shift alters the input statistics seen by each layer over time, making the optimization landscape more volatile and harder to navigate.\n",
    "\n",
    "**Normalization addresses this problem** by ensuring that the inputs to a layer maintain a consistent scale (typically with zero mean and unit variance). By normalizing the inputs ‚Äî either across the batch (BatchNorm), within each sample (LayerNorm), or across groups of features (GroupNorm) ‚Äî we help:\n",
    "\n",
    "- **Prevent exploding or vanishing gradients**\n",
    "- **Stabilize training** and make it more predictable\n",
    "- **Accelerate convergence** by allowing higher learning rates\n",
    "- **Improve generalization**, sometimes even acting like a regularizer\n",
    "\n",
    "The core idea is simple:  \n",
    "> ‚öôÔ∏è **Keep the inputs to each layer well-behaved**, because the output of one layer becomes the input to the next.  \n",
    "By maintaining a stable scale of activations throughout the network, normalization allows deep models to train efficiently and generalize better.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26d80c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üß† **Batch Normalization (BatchNorm)**\n",
    "\n",
    "BatchNorm **normalizes across the batch**, and it typically operates **per feature/channel** (not across features like LayerNorm).\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Example: Let's say you have an input of shape:\n",
    "\n",
    "For NLP (e.g., transformer input):\n",
    "```\n",
    "(batch_size, seq_len, hidden_dim)\n",
    "```\n",
    "\n",
    "For CNNs (e.g., images):\n",
    "```\n",
    "(batch_size, channels, height, width)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ BatchNorm Steps (Training Phase):\n",
    "\n",
    "For **each feature dimension** `d` (in NLP) or **each channel** (in CNN), BatchNorm computes:\n",
    "\n",
    "$$\n",
    "\\mu_d = \\frac{1}{N} \\sum_{i=1}^{N} x_i^d, \\quad \\sigma_d = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (x_i^d - \\mu_d)^2 + \\epsilon }\n",
    "$$\n",
    "\n",
    "- Here, \\( N \\) is the number of values in the batch (and possibly across spatial dims too, like height and width in CNNs)\n",
    "- \\( x_i^d \\) is the value of feature `d` for the `i`th data point\n",
    "- This means: for each feature/channel `d`, normalize using the **mean and std of that feature across the batch**\n",
    "\n",
    "$$\n",
    "\\text{BatchNorm}(x^d) = \\gamma_d \\cdot \\frac{x^d - \\mu_d}{\\sigma_d} + \\beta_d\n",
    "$$\n",
    "\n",
    "> **Important:** During training, these batch-wise statistics are also used to update **running estimates** of the mean and variance for each feature using exponential moving averages.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ BatchNorm at **Inference Time**\n",
    "\n",
    "During inference, we **do not compute statistics from the incoming batch**, because:\n",
    "- The batch size may be small (even size 1)\n",
    "- We want consistent behavior\n",
    "\n",
    "Instead, we use the **running estimates** of mean and variance accumulated during training:\n",
    "\n",
    "$$\n",
    "\\text{mean}_{\\text{new}} = (1 - \\alpha) \\cdot \\text{mean}_{\\text{old}} + \\alpha \\cdot \\text{mean}_{\\text{batch}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{var}_{\\text{new}} = (1 - \\alpha) \\cdot \\text{var}_{\\text{old}} + \\alpha \\cdot \\text{var}_{\\text{batch}}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- \\( mean_new \\) and \\( var_new \\) are the **running mean and std** (or variance) tracked during training\n",
    "- These are stored in the layer and **frozen** during inference\n",
    "\n",
    "---\n",
    "\n",
    "### üö® Key Differences Between BatchNorm and LayerNorm\n",
    "\n",
    "| Property              | **BatchNorm**                                                  | **LayerNorm**                                      |\n",
    "|----------------------|----------------------------------------------------------------|---------------------------------------------------|\n",
    "| Normalizes over      | **Batch dimension** (per feature/channel)                      | **Feature dimension** (per sample)                |\n",
    "| Statistics used       | Batch-wise mean/variance (training) or running stats (inference) | Sample-wise mean/variance (always)              |\n",
    "| Sensitive to batch size? | ‚úÖ Yes ‚Äî small batch sizes can cause instability           | ‚ùå No ‚Äî works even with batch size 1              |\n",
    "| Used in              | CNNs, older RNNs                                               | Transformers, newer RNNs, all NLP models          |\n",
    "| Behavior at inference| Uses **stored running averages** of mean/variance              | Recomputes from current input each time           |\n",
    "| Learnable params     | Œ≥ and Œ≤ per feature/channel                                    | Œ≥ and Œ≤ per feature                               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b44026",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "In **Layer Normalization**, we **do *not*** normalize across the *sequence/time steps*. Instead, **we normalize across the features/dimensions within a single input vector**, *for each element in the batch independently*.\n",
    "\n",
    "### Let's break it down properly:\n",
    "\n",
    "Suppose your input tensor is shaped like:\n",
    "\n",
    "```\n",
    "(batch_size, seq_len, hidden_dim)\n",
    "```\n",
    "\n",
    "LayerNorm is applied **at each time step independently**, and **across the `hidden_dim`** ‚Äî not across sequence or batch.\n",
    "\n",
    "So for a single vector:\n",
    "```\n",
    "x = [x‚ÇÅ, x‚ÇÇ, ..., x_d]  ‚àà ‚Ñù^d   (this is one hidden vector, e.g. for one time step)\n",
    "```\n",
    "\n",
    "LayerNorm computes:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i, \\quad \\sigma = \\sqrt{ \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2 + \\epsilon }\n",
    "$$\n",
    "\n",
    "Then the normalized output is:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "$$\n",
    "\n",
    "- `Œ≥` (gamma) and `Œ≤` (beta) are **learnable parameters**, one per feature dimension (`d` total)\n",
    "- `‚àò` is element-wise multiplication\n",
    "\n",
    "So the correct way to say it is:\n",
    "\n",
    "> In LayerNorm, we take **each individual input vector** (e.g. per time step), and normalize **across its features** using the formula:\n",
    ">\n",
    "> $$\n",
    "> \\text{output} = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "> $$\n",
    ">\n",
    "> This ensures that the normalized vector has zero mean and unit variance **per vector**, and the learnable `Œ≥` and `Œ≤` allow the network to re-scale and shift as needed.\n",
    "\n",
    "- In layer normalization there is no need to maintain running mean and variance as we are normalizing it along the features. so can be used easily during inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cedf83",
   "metadata": {},
   "source": [
    "## Question ?\n",
    "---\n",
    "\n",
    "> In the case of Batch Normalization, the idea makes statistical sense ‚Äî we compute the mean and variance over a mini-batch for each feature dimension, effectively treating the batch as a sample to prevent scale instability across features. This aligns well with the statistical principle that normalization should be based on a group of observations.\n",
    ">\n",
    "> However, in Layer Normalization, we compute the mean and variance across all feature dimensions of a single input vector (e.g., a 512-dimensional embedding), without aggregating across multiple samples. From a statistical standpoint, this seems less intuitive, since we're not sampling over a population or batch. Doesn't this approach violate the principle that normalization should rely on a sample of values to estimate meaningful statistics?\n",
    ">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efa9ef",
   "metadata": {},
   "source": [
    "### Why ‚Äúnormalize across the *features* of one vector‚Äù actually helps\n",
    "LayerNorm was introduced (Ba et al., 2016) after people tried BatchNorm inside RNNs and discovered two practical road-blocks:\n",
    "\n",
    "| Issue with Batch Statistics | What happens in RNNs/Transformers |\n",
    "|---|---|\n",
    "| **Statistics depend on other samples.**  A token at position *t* sees a different mean/var when the mini-batch composition changes. | The hidden state of token *t* is repeatedly reused at every time-step / attention block; if its scale keeps drifting, gradients explode or vanish. |\n",
    "| **Small or varying batch sizes are common.**  BatchNorm gets noisy or unusable when `batch_size‚âà1`. | Autoregressive decoding, on-device inference, curriculum learning, very long sequences ‚Üí batches can be tiny, even size 1. |\n",
    "\n",
    "LayerNorm side-steps both problems by **treating each hidden vector as its own ‚Äúmini-batch‚Äù.**\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition ‚ë† ‚Äì keep each vector on a well-behaved manifold\n",
    "Every hidden vector **h ‚àà ‚Ñù<sup>d</sup>** (e.g. `d = 512`) is pushed onto a *learnable hyper-ellipsoid*:\n",
    "\n",
    "$$\n",
    "\\tilde h = \\gamma \\odot \\frac{h-\\mu}{\\sigma} + \\beta ,\n",
    "\\qquad \n",
    "\\mu=\\tfrac1d\\sum_{i=1}^{d}h_i ,\n",
    "\\quad \n",
    "\\sigma=\\sqrt{\\tfrac1d\\sum_{i=1}^{d}(h_i-\\mu)^2+\\varepsilon}.\n",
    "$$\n",
    "\n",
    "- **Zero-centre & unit-variance** ‚áí dot-products, softmax logits, gating sigmoids, etc. all see inputs of comparable scale.  \n",
    "- **Œ≥, Œ≤** immediately restore any scale/shift the next layer *wants*, so we don‚Äôt lose representation power; we only stop *uncontrolled drift*.\n",
    "\n",
    "Think of it as replacing ‚Äúraw space‚Äù with **a coordinate system whose axes have the same statistical weight at every step.**\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition ‚ë° ‚Äì self-attention is a massive dot-product machine  \n",
    "In transformers the core operation is  \n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\Bigl(\\tfrac{QK^\\top}{\\sqrt{d_k}}\\Bigr)V .\n",
    "$$\n",
    "\n",
    "If individual token vectors explode, the *scale* of \\(QK^\\top\\) explodes ‚áí vanishing gradients through the softmax.  \n",
    "LayerNorm keeps each vector length roughly the same so that the \\(\\tfrac1{\\sqrt{d_k}}\\) factor truly stabilises things.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition ‚ë¢ ‚Äì per-sample whitening works like adaptive learning-rate  \n",
    "For a single linear layer \\(y=W h\\), the gradient w.r.t. \\(h_i\\) is proportional to \\(W_{:,i}\\).  \n",
    "If some coordinates of \\(h\\) are routinely 10√ó larger, their gradients are 10√ó larger too ‚áí training becomes imbalanced.  \n",
    "LayerNorm rescales *inside the network* so every coordinate learns at a comparable speed, similar to Adam‚Äôs per-parameter ‚àövariance term but embedded in the forward pass.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ÄúBut aren‚Äôt 512 features too few to estimate a variance?‚Äù\n",
    "We‚Äôre **not** trying to estimate the *population* variance of some random variable; we‚Äôre enforcing a *constraint* on *this particular vector*.  \n",
    "Per-feature fluctuations between tokens are exactly what we want to damp: if a token suddenly gets a huge value in one dimension because of an unlucky softmax, LayerNorm reins it in *immediately*, without waiting for other samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Putting it back in NLP context\n",
    "1. **Embedding lookup** already gives you heavy-tailed coordinates (some tokens use rare sub-spaces).  \n",
    "2. Feed-forward + attention layers mix those coordinates non-linearly; their scale can drift over depth.  \n",
    "3. LayerNorm ‚Äúflushes‚Äù that drift at every block, making depth-wise residual addition (`x + F(x)`) numerically safe.  \n",
    "4. Because it‚Äôs batch-agnostic, you can:\n",
    "   * run sequence-by-sequence during inference,\n",
    "   * fine-tune with very small batches,\n",
    "   * shuffle tokens freely without changing behaviour.\n",
    "\n",
    "That is why almost every transformer variant since 2017 uses LayerNorm (often twice per block), while BatchNorm virtually disappeared from sequence models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b5613",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick mental picture\n",
    "\n",
    "```\n",
    "BatchNorm:  ‚Üêmean/var over 32 samples‚Üí  (per dim)   =====>  stabilise feature *population*\n",
    "LayerNorm:  ‚Üêmean/var over 512 dims‚Üí    (per sample) =====>  stabilise *each* sample‚Äôs vector\n",
    "```\n",
    "\n",
    "Different axes, same core goal: **smooth training by taming activation scale‚Äîjust tailored to the setting (images vs. sequences).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdd064",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc06e433",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üîÅ **What Are Residual (Skip) Connections in Transformers?**\n",
    "\n",
    "In Transformer architectures, **residual connections** are used to **bypass** a layer's transformation and directly **add** the layer's input to its output.\n",
    "\n",
    "Formally, for a sub-layer (like self-attention or feed-forward layer), the residual connection is:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `x` is the input to the sub-layer,\n",
    "- `Sublayer(x)` is the transformation (like attention or MLP),\n",
    "- `x + Sublayer(x)` is the residual connection (a skip),\n",
    "- `LayerNorm` is applied **after** the addition (in post-LN transformers; some variants use pre-LN).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **How Residual Connections Help Gradient Flow**\n",
    "\n",
    "The main benefit comes during **backpropagation** ‚Äî when gradients are propagated from output back to input. Let's see **why they're useful in deep networks**:\n",
    "\n",
    "#### üîª Without Skip Connections:\n",
    "- Suppose you have a very deep network, say 48 layers.\n",
    "- Each layer applies a nonlinear transformation (e.g., ReLU, attention, etc.).\n",
    "- If each transformation reduces the gradient just a little (say by multiplying with a small value), then **after many layers**, the gradients can shrink to almost zero ‚Üí **vanishing gradients**.\n",
    "- Result: early layers stop learning.\n",
    "\n",
    "#### üîÑ With Skip Connections:\n",
    "- The gradient has a **direct path** to flow backward through the skip.\n",
    "- Since the output is \\( x + \\text{F}(x) \\), the gradient of the loss \\( L \\) w.r.t. \\( x \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\left(1 + \\frac{\\partial F(x)}{\\partial x}\\right)\n",
    "$$\n",
    "\n",
    "- The presence of that **`1`** ensures that **some part of the gradient always flows unchanged**.\n",
    "- This allows **deeper layers to still receive meaningful gradients**, even when `F(x)` becomes small or vanishes.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why It Works So Well\n",
    "\n",
    "1. **Identity Path:** The skip connection behaves like an identity map ‚Äî gradients flow even if the main layer `F(x)` is untrained or weakly contributing.\n",
    "2. **Gradient Preservation:** Each layer adds a small delta on top of identity ‚Äî like learning *residuals*, not full representations.\n",
    "3. **Ease of Optimization:** The network is encouraged to learn modifications to identity rather than new functions from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ In Transformers Specifically:\n",
    "- Each attention and feed-forward layer has a residual path.\n",
    "- This is critical because transformers are **deep stacks of blocks**.\n",
    "- Without residuals, models like GPT-3, BERT, or ViT wouldn't train effectively due to **gradient degradation**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca8523",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
