{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5006f8f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧠 Additive Attention (Bahdanau Attention) \n",
    "\n",
    "- Developed to overcome the fixed-length context bottleneck in encoder-decoder RNN/LSTM models.\n",
    "- Instead of relying on a single vector for the entire input sequence, the decoder **attends to all encoder hidden states** while generating each output token.\n",
    "- For each output time step \\( i \\), the decoder computes a **context vector** \\( \\mathbf{c}_i \\) as a **weighted sum of encoder annotations** \\( \\mathbf{h}_j \\):\n",
    "\n",
    "  $$\n",
    "  \\mathbf{c}_i = \\sum_{j=1}^{T_x} \\alpha_{ij} \\mathbf{h}_j \\tag{5}\n",
    "  $$\n",
    "\n",
    "- The attention weights \\( \\alpha_{ij} \\) reflect the relevance of each encoder hidden state \\( \\mathbf{h}_j \\) to the current decoder state \\( \\mathbf{s}_{i-1} \\):\n",
    "\n",
    "  $$\n",
    "  \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})} \\tag{6}\n",
    "  $$\n",
    "\n",
    "- The **alignment score** \\( e_{ij} \\) is computed using a small feedforward neural network (the **alignment model**) that takes \\( \\mathbf{s}_{i-1} \\) and \\( \\mathbf{h}_j \\) as inputs:\n",
    "\n",
    "  $$\n",
    "  e_{ij} = \\mathbf{v}^T \\tanh(\\mathbf{W}_1 \\mathbf{s}_{i-1} + \\mathbf{W}_2 \\mathbf{h}_j + \\mathbf{b}) \\tag{7}\n",
    "  $$\n",
    "\n",
    "  - v: learnable weight vector\n",
    "  - W_1, W_2: learnable weight matrices\n",
    "  - b : bias term\n",
    "\n",
    "- This is called **additive attention** because the alignment model uses **addition** (not dot product) to combine the decoder and encoder hidden states.\n",
    "\n",
    "- The attention mechanism allows the decoder to dynamically **focus on relevant parts** of the input sequence during each generation step.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f11a1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ✅ In **Dot-Product Attention (Luong)** used in RNN-based encoder-decoder:\n",
    "\n",
    "- The **basic variant** computes the score as:\n",
    "  $$\n",
    "  e_{ij} = \\mathbf{s}_i^\\top \\mathbf{h}_j\n",
    "  $$\n",
    "  where:\n",
    "  - s_i: decoder hidden state at time step \\( i \\)\n",
    "  - h_j: encoder hidden state at time step \\( j \\)\n",
    "\n",
    "- This is **just a dot product** — **no learnable parameters** involved in this scoring step.\n",
    "\n",
    "---\n",
    "\n",
    "### Why use dot-product attention then?\n",
    "\n",
    "- ✅ **Efficiency**: It’s faster — no extra parameters or matrix ops.\n",
    "- ❌ **Less expressive**: Cannot learn complex alignments or transformations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4aaa9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔁 Limitation in RNN-based Attention Mechanisms (Both Additive & Dot-Product)\n",
    "\n",
    "- In both **additive attention (Bahdanau)** and **dot-product attention (Luong)**, attention is computed **from the decoder to the encoder**:\n",
    "  - The decoder focuses on different encoder hidden states while generating each output.\n",
    "  - But there is **no attention among encoder tokens themselves**.\n",
    "  \n",
    "- ⚠️ That means:\n",
    "  - **Input tokens do not attend to each other.**\n",
    "  - The encoder processes the input sequentially using RNNs (or LSTMs), and the final hidden states are used in attention.\n",
    "  - This limits the model’s ability to capture **long-range dependencies** and **global interactions** within the input sequence.\n",
    "\n",
    "- ✅ This limitation was addressed by the **Transformer model** (Vaswani et al., 2017), which introduced:\n",
    "  - **Self-attention**: each token in the input sequence **attends to all other tokens**, capturing global context.\n",
    "  - **Positional encoding**: to retain sequence order, since self-attention is not sequential like RNNs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd70ee7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔍 Why RNNs (even Bidirectional) Still Struggled to Capture Global Token Relations\n",
    "\n",
    "#### ✅ What Bidirectional RNNs *do*:\n",
    "- A **unidirectional RNN/LSTM** only sees the past: for time step \\( t \\), it processes from \\( x_1 \\) to \\( x_t \\).\n",
    "- A **bidirectional RNN (BiRNN)** adds another RNN that processes in reverse: from \\( x_T \\) to \\( x_1 \\).\n",
    "- So at each time step \\( t \\), the hidden state \\( h_t \\) contains information from both past and future tokens — i.e., **local context** from both directions.\n",
    "\n",
    "#### ❌ What they *still can't do well*:\n",
    "- Even with both directions, **each hidden state still represents only a compressed summary** of the context — and that compression is:\n",
    "  - **Sequential**\n",
    "  - **Fixed-size**\n",
    "  - **Hard to optimize over long sequences**\n",
    "- **No direct interaction** between distant tokens unless the information is passed **step-by-step**, which leads to:\n",
    "  - **Vanishing gradients**\n",
    "  - **Poor long-range dependency modeling**\n",
    "- For example, the relationship between \\( x_3 \\) and \\( x_{98} \\) has to be encoded **indirectly** through dozens of hidden states in between.\n",
    "\n",
    "---\n",
    "\n",
    "### 🤝 Why Attention Helped:\n",
    "- Attention allows the model to **look at all tokens at once**, not just rely on the final compressed hidden state.\n",
    "- But in **encoder-decoder attention**, the **input tokens (encoder hidden states) are never allowed to look at each other**.\n",
    "  - The decoder attends to the encoder outputs.\n",
    "  - But **encoder tokens don’t attend to other encoder tokens** — each hidden state is still generated sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Transformers: Game-Changer\n",
    "- Introduced **self-attention** in both encoder and decoder.\n",
    "- Now, **every token attends to every other token directly** (not through hidden states).\n",
    "- This enables the model to:\n",
    "  - Capture **global dependencies directly**\n",
    "  - Be **parallelizable** (no need for sequential processing)\n",
    "  - Better handle **long-range interactions**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 TL;DR:\n",
    "> Even bidirectional RNNs process sequences sequentially. They encode context *around* a token, but **don't model direct pairwise interactions** between all tokens. That’s why they can miss global relationships — especially over long distances. Attention fixed this for decoder → encoder, and Transformers fixed it fully with **self-attention**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28312c05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411e78d8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔁 Self-Attention & Contextualized Embeddings\n",
    "\n",
    "- **Self-attention** (or **intra-attention**) is a mechanism where:\n",
    "  - Each token in a sequence **attends to all other tokens**, including itself.\n",
    "  - It calculates **how much focus** to place on other tokens when updating its own representation.\n",
    "\n",
    "- The result is a **new embedding** for each token — called a **contextualized embedding**.\n",
    "\n",
    "- Unlike static embeddings (e.g., Word2Vec, GloVe), which assign **one fixed vector per word**, self-attention allows the **same word to have different embeddings depending on context**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Context Matters — The “Apple” Example\n",
    "\n",
    "Imagine these two sentences:\n",
    "\n",
    "1. **\"I ate a juicy apple after lunch.\"** 🍎  \n",
    "2. **\"I updated my Apple phone to the latest version.\"** 📱\n",
    "\n",
    "- In both cases, the word **\"apple\"** is spelled the same.\n",
    "- But their **meanings are completely different** — fruit vs tech company.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ How Self-Attention Helps:\n",
    "\n",
    "| Sentence                              | Context Around \"Apple\"         | Resulting Embedding          |\n",
    "|---------------------------------------|--------------------------------|------------------------------|\n",
    "| \"I ate a juicy **apple** after lunch\" | Tokens like *ate*, *juicy*, *lunch* | Embedding leans toward **fruit** 🍏 |\n",
    "| \"I updated my **Apple** phone...\"     | Tokens like *updated*, *phone*, *version* | Embedding leans toward **technology** 📱 |\n",
    "\n",
    "- Thanks to **self-attention**, \"apple\" attends to words around it.\n",
    "- The model **learns the meaning from context**, so the embedding for \"apple\" becomes **context-aware**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Key Benefits of Contextualized Embeddings:\n",
    "\n",
    "- 🔍 **Disambiguation**: Helps models distinguish between multiple meanings of the same word.\n",
    "- 🧠 **Rich understanding**: Better grasp of semantics and syntax.\n",
    "- 💬 **Improved performance**: Boosts accuracy in downstream tasks like translation, sentiment analysis, QA, etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3a3e4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ Multi-Head Self-Attention\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "- **Embedding dimension** = 512  \n",
    "- **Number of heads** = 8  \n",
    "- Therefore, each head operates on **512 / 8 = 64** dimensions  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔢 Step-by-Step Breakdown\n",
    "\n",
    "#### 1. **Start with token input:**\n",
    "- You begin with the **input embedding**:\n",
    "  $$\n",
    "  \\text{input} = \\text{token embedding} + \\text{positional encoding} \\in \\mathbb{R}^{512}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Linear projections per head:**\n",
    "- We **project it into separate Q, K, V vectors for each head** using **separate weight matrices** for each head.\n",
    "  \n",
    "- So for **each head \\( i \\in [1, 8] \\)**:\n",
    "  $$\n",
    "  Q^{(i)} = X W_Q^{(i)}, \\quad K^{(i)} = X W_K^{(i)}, \\quad V^{(i)} = X W_V^{(i)}\n",
    "  $$\n",
    "  \n",
    "  where:\n",
    "  -  $$W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \\in \\mathbb{R}^{512 \\times 64}$$ \n",
    "  -  $$Q^{(i)}, K^{(i)}, V^{(i)} \\in \\mathbb{R}^{n \\times 64}$$ \n",
    "   for sequence length n \n",
    "\n",
    "✅ So yes: each matrix maps the **full 512-dim input** to a **64-dim subspace** for that head.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Self-attention for each head:**\n",
    "- Each head computes scaled dot-product attention **independently**:\n",
    "  $$\n",
    "  \\text{Attention}^{(i)} = \\text{softmax} \\left( \\frac{Q^{(i)} {K^{(i)}}^\\top}{\\sqrt{64}} \\right) V^{(i)}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Concatenation of heads:**\n",
    "- After computing attention outputs for all 8 heads (each of size \\( \\mathbb{R}^{n \\times 64} \\)), you **concatenate them** along the feature dimension:\n",
    "  $$\n",
    "  \\text{Concat} = [\\text{head}_1; \\text{head}_2; \\dots; \\text{head}_8] \\in \\mathbb{R}^{n \\times 512}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Final linear projection (W₀):**\n",
    "- You apply a final learned linear layer:\n",
    "  $$\n",
    "  \\text{Output} = \\text{Concat} \\cdot W_O\n",
    "  $$\n",
    "  where \\( W_O \\in \\mathbb{R}^{512 \\times 512} \\)\n",
    "\n",
    "- This projects the concatenated output **back to the original embedding dimension (512)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc299c9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔁 **Cross-Attention Mechanism**\n",
    "\n",
    "In **cross-attention**, the decoder attends to the encoder's output. Here's how the process works:\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧱 Components:\n",
    "- **Query (Q)** → from the **decoder's previous layer** (current decoding step).\n",
    "- **Key (K), Value (V)** → from the **encoder's output**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔣 Step-by-step Process:\n",
    "1. **Input:**\n",
    "   - Decoder hidden state → `Q` (Query)\n",
    "   - Encoder hidden states → `K`, `V` (Key, Value)\n",
    "\n",
    "2. **Compute Attention Scores:**\n",
    "   - Dot product: `Q ⋅ Kᵀ` → gives the **attention logits** (how much focus each decoder token puts on each encoder token).\n",
    "\n",
    "3. **Scale:**\n",
    "   - Divide by `√d_k` (dimension of key vectors) to prevent large values:\n",
    "     $$\n",
    "     \\text{Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "     $$\n",
    "\n",
    "4. **Softmax:**\n",
    "   - Apply softmax to get **attention weights** (sum to 1):\n",
    "     $$\n",
    "     \\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "     $$\n",
    "\n",
    "5. **Weighted Sum:**\n",
    "   - Multiply attention weights with `V`:\n",
    "     $$\n",
    "     \\text{Attention Output} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "     $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4466590",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧠 Time Complexity of Self-Attention\n",
    "\n",
    "Let:\n",
    "- \\( n \\): sequence length\n",
    "- \\( d \\): embedding dimension\n",
    "- \\( h \\): number of heads\n",
    "- \\( d_k = d/h \\): dimension per head (usually, \\( d_k \\approx 64 \\))\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 1. **Self-Attention (Single Head)**\n",
    "\n",
    "Self-attention requires the following operations per layer:\n",
    "\n",
    "#### 📌 Step-by-step breakdown:\n",
    "\n",
    "| Operation | Complexity |\n",
    "|----------|------------|\n",
    "| **Linear projections** (Q, K, V from input \\( X \\in \\mathbb{R}^{n \\times d} \\)) | \\( O(n \\cdot d^2) \\) |\n",
    "| **Dot product attention (QKᵀ)** | \\( O(n^2 \\cdot d) \\) |\n",
    "| **Softmax over \\( n \\) tokens** | \\( O(n^2) \\) |\n",
    "| **Multiply attention weights with V** | \\( O(n^2 \\cdot d) \\) |\n",
    "\n",
    "✅ **Total (Single-Head)**:  \n",
    "$$\n",
    "\\boxed{O(n^2 \\cdot d + n \\cdot d^2)}\n",
    "$$\n",
    "\n",
    "- \\( O(n^2 \\cdot d) \\): due to pairwise interactions for attention\n",
    "- \\( O(n \\cdot d^2) \\): due to the linear projections (Q, K, V)\n",
    "\n",
    "> The dominating term is usually **\\( O(n^2 \\cdot d) \\)** due to the attention matrix \\( QK^\\top \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔀 2. **Multi-Head Attention**\n",
    "\n",
    "In multi-head attention, we perform the attention operation **in parallel for each head**, with smaller dimensions \\( d_k = d/h \\), and then concatenate.\n",
    "\n",
    "So:\n",
    "- **Per head**: \\( O(n^2 \\cdot d_k + n \\cdot d \\cdot d_k) \\)\n",
    "- Across **\\( h \\)** heads: \\( h \\cdot O(n^2 \\cdot d_k) = O(n^2 \\cdot d) \\)\n",
    "\n",
    "Final projection (concatenated heads back to \\( d \\)-dim):\n",
    "- \\( O(n \\cdot d^2) \\)\n",
    "\n",
    "✅ **Total (Multi-Head)**:  \n",
    "$$\n",
    "\\boxed{O(n^2 \\cdot d + n \\cdot d^2)}\n",
    "$$\n",
    "\n",
    "> Same as single-head in big-O — but **more efficient in practice due to parallelism**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Bottleneck Insight\n",
    "\n",
    "- **Quadratic time complexity in sequence length \\( n \\)**:  \n",
    "  - the reason why Transformers become slow for long sequences (e.g. long documents or audio). \n",
    "    $$O(n^2 \\cdot d)$$ \n",
    "    \n",
    "  - **This comes from computing** \n",
    "    $$\n",
    "    QK^\\top \\in \\mathbb{R}^{n \\times n}\n",
    "    $$\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc44020",
   "metadata": {},
   "source": [
    "## Step-by-Step Time Complexity of Multi-Head Self-Attention\n",
    "\n",
    "Assume:\n",
    "- Sequence length: \\( n = 5 \\)\n",
    "- Embedding dimension: \\( d = 512 \\)\n",
    "- Number of attention heads: \\( h = 8 \\)\n",
    "- Dimension per head: \\( d_k = \\frac{d}{h} = 64 \\)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Linear Projections (Q, K, V)\n",
    "\n",
    "Each input token of dimension 512 is projected to Query, Key, and Value vectors:\n",
    "\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "Where:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{n \\times d}, \\quad W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\n",
    "$$\n",
    "\n",
    "Time Complexity:\n",
    "$$\n",
    "O(n \\cdot d^2) = 5 \\cdot 512^2 = O(1.3 \\times 10^6) \\text{ per projection}\n",
    "$$\n",
    "$$\n",
    "\\text{Total for Q, K, V: } 3 \\cdot O(n \\cdot d^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Scaled Dot Product — \\( QK^T \\)\n",
    "\n",
    "For each head:\n",
    "\n",
    "$$\n",
    "Q, K \\in \\mathbb{R}^{n \\times d_k} = \\mathbb{R}^{5 \\times 64}\n",
    "\\Rightarrow QK^\\top \\in \\mathbb{R}^{5 \\times 5}\n",
    "$$\n",
    "\n",
    "Time Complexity (1 head):\n",
    "$$\n",
    "O(n^2 \\cdot d_k) = 5^2 \\cdot 64 = 1600\n",
    "$$\n",
    "\n",
    "All heads:\n",
    "$$\n",
    "O(n^2 \\cdot d) = 25 \\cdot 512 = 12,800\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Softmax over Attention Scores\n",
    "\n",
    "Applied row-wise over \\( QK^\\top \\in \\mathbb{R}^{n \\times n} \\)\n",
    "\n",
    "Time Complexity:\n",
    "$$\n",
    "O(n^2) = 25 \\quad \\text{(negligible)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Weighted Sum with Value Matrix V\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot V \\quad \\text{where } \\alpha \\in \\mathbb{R}^{n \\times n}, V \\in \\mathbb{R}^{n \\times d_k}\n",
    "\\Rightarrow \\alpha V \\in \\mathbb{R}^{n \\times d_k}\n",
    "$$\n",
    "\n",
    "Time Complexity (1 head):\n",
    "$$\n",
    "O(n^2 \\cdot d_k) = 25 \\cdot 64 = 1600\n",
    "$$\n",
    "\n",
    "All heads:\n",
    "$$\n",
    "O(n^2 \\cdot d) = 25 \\cdot 512 = 12,800\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Concatenation + Final Linear Projection\n",
    "\n",
    "Concatenated output: \\( \\mathbb{R}^{n \\times d} = \\mathbb{R}^{5 \\times 512} \\)\n",
    "\n",
    "Final projection matrix: \\( W_O \\in \\mathbb{R}^{d \\times d} \\)\n",
    "\n",
    "Time Complexity:\n",
    "$$\n",
    "O(n \\cdot d^2) = 5 \\cdot 512^2 = O(1.3 \\times 10^6)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Total Time Complexity:\n",
    "\n",
    "$$\n",
    "\\boxed{O(n^2 \\cdot d + n \\cdot d^2)}\n",
    "$$\n",
    "\n",
    "This includes attention score computation and all linear projections. The quadratic term \\( n^2 \\cdot d \\) dominates for long sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547e2f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ab54ef2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 📐 Why Scale Dot Products in Attention by ? $$\\frac{1}{\\sqrt{d_k}}$$\n",
    "\n",
    "In **scaled dot-product attention**, we compute:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Let's break it down:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 The Root Cause: Variance of Dot Product Increases with Dimension\n",
    "\n",
    "- The dot product between two random vectors  $$q, k \\in \\mathbb{R}^{d_k}$$  \n",
    "  is:\n",
    "\n",
    "  $$\n",
    "  q \\cdot k = \\sum_{i=1}^{d_k} q_i k_i\n",
    "  $$\n",
    "\n",
    "- If each element \\( q_i \\) and \\( k_i \\) is sampled from a distribution with:\n",
    "  - Mean 0\n",
    "  - Variance $$\\sigma^2$$\n",
    "\n",
    "- Then by independence:\n",
    "  $$\n",
    "  \\text{Var}(q \\cdot k) = d_k \\cdot \\sigma^4\n",
    "  $$\n",
    "\n",
    "✅ **Conclusion**: As the dimensionality d_k increases, the **variance of the dot product also increases linearly**.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Why High Variance Is a Problem for Softmax\n",
    "\n",
    "- After computing QK^T, we apply the **softmax** to turn scores into probabilities:\n",
    "  \n",
    "  $$\n",
    "  \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_k \\exp(e_{ik})}\n",
    "  $$\n",
    "\n",
    "- Softmax is very sensitive to **large inputs**:\n",
    "  - A few large values dominate the output.\n",
    "  - This causes the attention distribution to become **very sharp** — i.e., **one token gets almost all the attention**, others are ignored.\n",
    "  - This leads to poor **gradient flow** during training (vanishing gradients for ignored tokens).\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 The Fix: Scale by $$\\frac{1}{\\sqrt{d_k}}$$\n",
    "\n",
    "- To control this, we scale the dot product by \\( \\frac{1}{\\sqrt{d_k}} \\), which reduces the variance:\n",
    "  \n",
    "  $$\n",
    "  \\text{Var}\\left( \\frac{q \\cdot k}{\\sqrt{d_k}} \\right) = \\frac{1}{d_k} \\cdot \\text{Var}(q \\cdot k) = \\sigma^4\n",
    "  $$\n",
    "\n",
    "- This makes the **variance independent of the dimension \\( d_k \\)**, ensuring more stable and balanced attention scores.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Intuition Behind Scaling\n",
    "\n",
    "- Scaling keeps the inputs to the softmax **within a reasonable range**.\n",
    "- This allows softmax to produce **more meaningful and smooth distributions**, rather than being dominated by a single high score.\n",
    "- Ensures better learning dynamics and **more diverse attention** across tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173d98a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
