{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37db0149",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üîπ **1. Masking in the Encoder**\n",
    "\n",
    "In the **encoder**, masking is mostly used for **padding tokens**:\n",
    "\n",
    "- **Padding Mask (or Attention Mask)**:\n",
    "  - Purpose: Prevents the model from attending to padded positions (`[PAD]`) that were added to make sequences the same length in a batch.\n",
    "  - Usage: Applied in both self-attention layers of the encoder and in encoder-decoder attention of the decoder.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **2. Masking in the Decoder**\n",
    "\n",
    "In the **decoder**, we use **two types of masks**:\n",
    "\n",
    "#### a. **Causal Mask (Look-Ahead Mask):**\n",
    "- Purpose: Prevents the decoder from \"cheating\" by looking at future tokens during training.\n",
    "- Implementation: Usually a triangular matrix where positions `[i][j]` are masked if `j > i`.\n",
    "- Effect: Each token can only attend to itself and previous tokens (i.e., autoregressive behavior).\n",
    "\n",
    "#### b. **Padding Mask (from encoder output):**\n",
    "- Purpose: When the decoder attends to the encoder's outputs (via encoder-decoder attention), this mask blocks attention to encoder‚Äôs padded tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30ab42",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. **Encoder Padding Mask Example**\n",
    "\n",
    "Let's say we have a batch with two sequences:\n",
    "```text\n",
    "Seq 1: [The, cat, sat, on, the, mat]\n",
    "Seq 2: [Dogs, bark, loudly, <PAD>, <PAD>, <PAD>]\n",
    "```\n",
    "\n",
    "#### ‚û§ Padding Mask for these:\n",
    "We mask out the `<PAD>` tokens:\n",
    "\n",
    "```python\n",
    "# Shape: [batch_size, 1, 1, seq_len]\n",
    "# 1 = keep (attend), 0 = mask (no attend)\n",
    "\n",
    "mask = [\n",
    "    [[1, 1, 1, 1, 1, 1]],       # No padding\n",
    "    [[1, 1, 1, 0, 0, 0]]        # Mask last 3 positions\n",
    "]\n",
    "```\n",
    "\n",
    "This will be **broadcasted** during self-attention to ignore contributions from padding tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. **Decoder Causal (Look-Ahead) Mask Example**\n",
    "\n",
    "Say the target sequence is:\n",
    "```text\n",
    "[\"I\", \"love\", \"you\"]\n",
    "```\n",
    "\n",
    "We don‚Äôt want \"love\" to see \"you\" during training. So, the **look-ahead mask** is:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "mask = torch.triu(torch.ones(3, 3), diagonal=1)\n",
    "# Now set masked positions to -inf\n",
    "mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, 0.0)\n",
    "```\n",
    "\n",
    "#### ‚û§ Resulting mask (before softmax):\n",
    "```\n",
    "[[  0.  -inf  -inf]\n",
    " [  0.    0.  -inf]\n",
    " [  0.    0.    0.]]\n",
    "```\n",
    "\n",
    "When softmax is applied row-wise:\n",
    "- `-inf` becomes **0 probability**\n",
    "- Valid positions have softmax computed normally.\n",
    "\n",
    "#### ‚û§ Visual (Heatmap-style):\n",
    "\n",
    "| Pos \\ Attn to | 0 (I) | 1 (love) | 2 (you) |\n",
    "|---------------|--------|----------|----------|\n",
    "| 0             | ‚úÖ     | ‚ùå       | ‚ùå       |\n",
    "| 1             | ‚úÖ     | ‚úÖ       | ‚ùå       |\n",
    "| 2             | ‚úÖ     | ‚úÖ       | ‚úÖ       |\n",
    "\n",
    "This enforces autoregressive prediction: each token can only attend to earlier tokens and itself.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e363487",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
