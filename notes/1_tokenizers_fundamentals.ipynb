{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951d5507",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ‚úÖ Tokenizers:\n",
    "\n",
    "- **Tokenization is the first step** in any Transformer-based Natural Language Processing (NLP) pipeline.\n",
    "- **Neural networks cannot process raw text**, so tokenization converts human-readable text into **numerical token IDs** that models can understand.\n",
    "- An effective tokenizer must balance:\n",
    "  - **Preserving the semantic meaning** of the text.\n",
    "  - **Keeping the vocabulary size manageable** for efficient training and generalization.\n",
    "- The choice of tokenizer has a **direct impact on the model‚Äôs performance**, especially in tasks like text classification, translation, and question answering.\n",
    "- **Subword-based tokenizers** (e.g., BPE, WordPiece, Unigram LM) are preferred over **word-level** or **character-level** approaches because they:\n",
    "  - Reduce the out-of-vocabulary (OOV) problem.\n",
    "  - Handle **rare and compound words** more effectively.\n",
    "  - Support **multilingual** and domain-specific tasks better.\n",
    "  - Produce **compact and generalizable representations**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff8730",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß© Why Tokenization Matters\n",
    "\n",
    "1. **Efficiency** ‚Äì Subword tokens balance sequence length and vocabulary size.\n",
    "2. **Generalization** ‚Äì Models can handle rare/unseen words by breaking them down.\n",
    "3. **Multilingual Support** ‚Äì Subword tokenization helps handle multiple languages with shared vocab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61af516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['transformers', 'are', 'revolution', '##izing', 'natural', 'language', 'processing', '.']\n",
      "Token IDs: [19081, 2024, 4329, 6026, 3019, 2653, 6364, 1012]\n",
      "Encoded with special tokens: [101, 19081, 2024, 4329, 6026, 3019, 2653, 6364, 1012, 102]\n",
      "Decoded text: [CLS] transformers are revolutionizing natural language processing. [SEP]\n"
     ]
    }
   ],
   "source": [
    "## This is how tokenization is done\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sample sentence\n",
    "text = \"Transformers are revolutionizing natural language processing.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Alternatively, use the encode method to get IDs directly (includes special tokens like [CLS], [SEP])\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"Encoded with special tokens:\", encoded)\n",
    "\n",
    "# Decode back to text to see how the model interprets the ID sequence\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded text:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cb66d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### ‚ùì **Question:**  \n",
    "Earlier, for generating embeddings from text, we had models like Word2Vec (CBOW, Skip-Gram) and GloVe. Were tokenizers used at that time as well, or did tokenizers come into existence only after transformer models?\n",
    "\n",
    "---\n",
    "---\n",
    "### ‚úÖ **Answer:**  \n",
    "Yes, tokenizers were used even before transformers ‚Äî they are not new. However, the **type and complexity** of tokenization used in models like Word2Vec and GloVe were much **simpler** compared to those used in modern transformer models.\n",
    "\n",
    "- **Pre-transformer models** (like Word2Vec, CBOW, Skip-Gram, and GloVe) generally used **basic word-level tokenization**, such as splitting text on whitespace or punctuation.\n",
    "- These models treated each **word** as a distinct unit and learned **static embeddings** for them.\n",
    "- If a word wasn‚Äôt in the vocabulary, it was considered **out-of-vocabulary (OOV)** and either ignored or replaced with a special token like `[UNK]`.\n",
    "\n",
    "In contrast, **transformer-based models** (like BERT and GPT) required more advanced tokenization methods due to:\n",
    "- The need to handle **rare and unseen words**\n",
    "- **Multilingual and domain-specific** text\n",
    "- **Longer context windows** and **contextual embeddings**\n",
    "\n",
    "Hence, tokenizers like **Byte Pair Encoding (BPE)**, **WordPiece**, and **Unigram Language Model** were introduced to tokenize at the **subword** level and build a more **robust, flexible vocabulary**.\n",
    "\n",
    "So, while tokenization **did exist before transformers**, the **modern, learned subword tokenization** techniques were introduced with and for transformers.\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af38a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üï∞Ô∏è **Early Tokenization Approaches (Pre-Transformer Era)**\n",
    "\n",
    "### üîπ Word-level Tokenization\n",
    "- Example: `\"I love machine learning\"` ‚Üí `[\"I\", \"love\", \"machine\", \"learning\"]`\n",
    "- Each word gets a unique ID.\n",
    "- **Problem**: Vocabulary size is huge, and unseen words (out-of-vocabulary or OOV) are a big problem.\n",
    "\n",
    "### üîπ Character-level Tokenization\n",
    "- Example: `\"hello\"` ‚Üí `[\"h\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "- Helps with OOV, but **loses semantic meaning** at word/subword level.\n",
    "- Input sequences become longer, slowing training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ **Limitations of Word-Based Tokenizers**\n",
    "\n",
    "- **Massive Vocabulary Requirement:**\n",
    "  - To fully cover a language like English, a word-based tokenizer needs an **identifier for every possible word**.\n",
    "  - English alone has over **500,000 words**, leading to a **huge vocabulary size** and memory footprint.\n",
    "\n",
    "- **No Generalization Across Similar Words:**\n",
    "  - Words like `\"dog\"` and `\"dogs\"` or `\"run\"` and `\"running\"` are treated as **completely separate tokens**.\n",
    "  - The model has **no inherent understanding** of their similarity, unless it learns it during training.\n",
    "  - This leads to **inefficient learning**, especially for morphological variants of the same root word.\n",
    "\n",
    "- **Out-of-Vocabulary (OOV) Problem:**\n",
    "  - Words not present in the tokenizer's vocabulary are replaced with a special **‚Äúunknown‚Äù token**, commonly `[UNK]` or `<unk>`.\n",
    "  - This results in a **loss of information**, as the model receives **no meaningful representation** of the word.\n",
    "  - If many `[UNK]` tokens appear, it's a sign that the tokenizer is **failing to capture the input effectively**.\n",
    "\n",
    "- **Vocabulary Design Trade-off:**\n",
    "  - A larger vocabulary reduces `[UNK]` usage but **increases memory and computation**.\n",
    "  - A smaller vocabulary leads to more `[UNK]` tokens, causing **information loss**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **Limitations of Character-Level Tokenization**\n",
    "\n",
    "- **Reduced Semantic Meaning (Especially in Latin Languages):**\n",
    "  - Characters carry **less intuitive meaning** on their own compared to full words.\n",
    "  - For example, the letter `\"e\"` has little meaning by itself, unlike a word like `\"eat\"`.\n",
    "\n",
    "- **Language Dependency:**\n",
    "  - In languages like **Chinese**, each character often represents a full word or concept, making **character-level tokenization more meaningful**.\n",
    "  - In contrast, for **Latin-based languages** (like English), individual characters provide **less useful context**.\n",
    "\n",
    "- **Longer Sequences:**\n",
    "  - A single word that would normally be **one token** using a word-based or subword-based tokenizer can become **10 or more tokens** at the character level.\n",
    "  - This leads to:\n",
    "    - **Increased computational cost**\n",
    "    - **Higher memory usage**\n",
    "    - Potential for **longer training and inference times**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c24265",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß¨ **Subword Tokenization ‚Äî The Game Changer**\n",
    "\n",
    "Subword tokenization is a **middle ground** between **word-level** & **character-level** tokenization. It breaks words into smaller **frequent units**.\n",
    "\n",
    "---\n",
    "\n",
    "### üî† **Why Subword Tokenization Works Well**\n",
    "\n",
    "- **Core Principle:**\n",
    "  - Subword tokenization relies on the idea that:\n",
    "    - **Frequent words** should be kept **as whole tokens**.\n",
    "    - **Rare words** should be **split into meaningful subword units**.\n",
    "\n",
    "- **Example:**\n",
    "  - The word `\"annoyingly\"` might be split into:\n",
    "    - `\"annoying\"` and `\"ly\"`\n",
    "    - Both are common subwords with individual meanings.\n",
    "    - The **composite meaning** is preserved while improving token efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **Tokenization Example:**\n",
    "\n",
    "Given the sentence:  \n",
    "**\"Let‚Äôs do tokenization!\"**\n",
    "\n",
    "- A subword tokenizer might split it as:\n",
    "  ```\n",
    "  [\"Let\", \"‚Äô\", \"s\", \"do\", \"token\", \"ization\", \"!\"]\n",
    "  ```\n",
    "\n",
    "- Key observations:\n",
    "  - `\"token\"` and `\"ization\"` are semantically meaningful.\n",
    "  - These **subwords reduce the total number of tokens** while preserving meaning.\n",
    "  - **Long words get efficiently represented** without adding `[UNK]`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advantages of Subword Tokenization:**\n",
    "\n",
    "- **Efficient Vocabulary Coverage:**\n",
    "  - Enables **compact vocabularies** while still representing a vast number of word forms.\n",
    "  - Reduces the occurrence of **unknown tokens** (e.g., `[UNK]`).\n",
    "\n",
    "- **Preserves Semantic Information:**\n",
    "  - Splits maintain **interpretable pieces** of meaning (e.g., `\"token\"` + `\"ization\"`).\n",
    "  - Helps the model **learn better contextual representations**.\n",
    "\n",
    "- **Highly Effective in Agglutinative Languages:**\n",
    "  - Languages like **Turkish** or **Finnish** allow long, complex words formed by stringing smaller parts.\n",
    "  - Subword tokenization handles these gracefully without needing massive vocabularies.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a275fc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß© **Understanding Normalization and Pre-tokenization in Transformers**\n",
    "\n",
    "Before diving into subword tokenization algorithms like **Byte Pair Encoding (BPE)**, **WordPiece**, and **Unigram**, it's important to understand two essential preprocessing steps that all tokenizers perform:\n",
    "\n",
    "---\n",
    "\n",
    "### üßº **1. Normalization**\n",
    "\n",
    "**Purpose:**  \n",
    "Cleans up and standardizes the text( **not splitted yet** ) before tokenization. It ensures that different textual variations map to the same base form.\n",
    "\n",
    "**Common operations include:**\n",
    "- Lowercasing (in uncased models)\n",
    "- Removing accents (diacritics)\n",
    "- Unicode normalization (e.g., NFC, NFKC)\n",
    "- Stripping extra whitespace\n",
    "\n",
    "---\n",
    "\n",
    "#### üî† **1.1. Lowercasing (in uncased models)**  \n",
    "Converts all characters to lowercase for case-insensitive models.\n",
    "\n",
    "- **Input:** `\"Transformers are AWESOME!\"`  \n",
    "- **Output:** `\"transformers are awesome!\"`  \n",
    "- ‚úÖ Helps reduce vocabulary size and treats `\"Dog\"` and `\"dog\"` as the same.\n",
    "\n",
    "---\n",
    "\n",
    "#### üá¶üá® **1.2. Removing Accents (Diacritics)**  \n",
    "Strips accents from letters to handle accented and unaccented forms as equivalent.\n",
    "\n",
    "- **Input:** `\"H√©llo, h√≥w √†r√© y√∂u?\"`  \n",
    "- **Output:** `\"Hello, how are you?\"`  \n",
    "- ‚úÖ Useful in multilingual settings where accents may vary or be inconsistently typed.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî° **1.3. Unicode Normalization (e.g., NFC, NFKC)**  \n",
    "Ensures that characters with multiple Unicode representations are treated as equivalent.\n",
    "\n",
    "> üí° **Every character in digital text is represented using a unique Unicode code point.** However, some characters can be represented in **multiple ways** in Unicode.\n",
    "\n",
    "For example, the character **\"√©\"** can be represented as:\n",
    "- A **single code point**: `U+00E9` (√©)\n",
    "- Or as **two combined code points**: `U+0065` (e) + `U+0301` (combining acute accent)\n",
    "\n",
    "- **Input:** `\"eÃÅcole\"` (using `e` + accent)\n",
    "- **Output after NFC normalization:** `\"√©cole\"` (single-character `√©`)\n",
    "\n",
    "‚úÖ Unicode normalization ensures both forms are treated as the same during processing.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßπ **1.4. Stripping Extra Whitespace**  \n",
    "Removes leading/trailing whitespace and reduces multiple spaces to a single one.\n",
    "\n",
    "- **Input:** `\"   Hello     world   \"`  \n",
    "- **Output:** `\"Hello world\"`  \n",
    "- ‚úÖ Keeps the text clean and avoids misleading token boundaries.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad226ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "## This is how normalizer of a Tokenizer can be used.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4146289",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úÇÔ∏è **2. Pre-tokenization**\n",
    "\n",
    "**Purpose:**  \n",
    "Breaks the normalized text into preliminary segments (like words, punctuation, or space tokens) before applying subword algorithms.\n",
    "\n",
    "**Why it matters:**  \n",
    "Tokenizers can't be trained on raw unsegmented text. Pre-tokenization helps define **boundaries** for subword learning.\n",
    "\n",
    "üîπ **Key Behavior:**\n",
    "- Splits on **whitespace and punctuation**\n",
    "- Collapses extra spaces\n",
    "- Keeps **offsets**, which are useful for alignment (e.g., in question answering)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c63116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]\n"
     ]
    }
   ],
   "source": [
    "## ***BERT Tokenizer*** ##\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5275e",
   "metadata": {},
   "source": [
    "\n",
    "### üî† **3. Tokenizer Differences in Pre-tokenization**\n",
    "\n",
    "#### üî∏ **GPT-2 Tokenizer**\n",
    "- Keeps whitespace by encoding it as a **special character `ƒ†`**\n",
    "- This allows reconstructing exact text formatting during decoding\n",
    "- Does **not ignore** double spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057411ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('ƒ†how', (6, 10)),\n",
       " ('ƒ†are', (10, 14)),\n",
       " ('ƒ†', (14, 15)),\n",
       " ('ƒ†you', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GPT2 Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a883bc9",
   "metadata": {},
   "source": [
    "#### üî∏ **T5 Tokenizer (SentencePiece-based)**\n",
    "- Uses **`‚ñÅ` (U+2581)** to represent space\n",
    "- Only splits on **whitespace**, not punctuation\n",
    "- Prepends a space automatically at the beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9460f3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅHello,', (0, 6)),\n",
       " ('‚ñÅhow', (7, 10)),\n",
       " ('‚ñÅare', (11, 14)),\n",
       " ('‚ñÅyou?', (16, 20))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54524bfa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ **Summary: Why These Steps Matter**\n",
    "\n",
    "These preprocessing stages are **crucial** for ensuring that tokenization is:\n",
    "- **Consistent**\n",
    "- **Reversible**\n",
    "- **Language- and domain-agnostic**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642b122",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîß **Byte-Pair Encoding (BPE) Tokenization**\n",
    "(HuggingFace Code Link: https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb )\n",
    "### üìò **1. Input Preprocessing and Pre-tokenization**\n",
    "\n",
    "- The raw text is first **pre-tokenized** ‚Äî usually split into individual words using whitespace and punctuation.\n",
    "- **Example Input Corpus:**\n",
    "  ```\n",
    "  \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
    "  ```\n",
    "\n",
    "- After pre-tokenization, the algorithm counts the **frequency** of each word in the corpus:\n",
    "  ```\n",
    "  (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **2. Initial Vocabulary Construction**\n",
    "\n",
    "- Each word is then **split into individual characters**:\n",
    "  ```\n",
    "  \"hug\" ‚Üí [\"h\", \"u\", \"g\"]\n",
    "  \"hugs\" ‚Üí [\"h\", \"u\", \"g\", \"s\"]\n",
    "  ```\n",
    "\n",
    "- All **unique characters** across the corpus are added to the **base vocabulary**:\n",
    "  ```\n",
    "  [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"[UNK]\"] \n",
    "  ```\n",
    "\n",
    "- This base vocabulary usually contains:\n",
    "  - All **ASCII characters**\n",
    "  - Possibly **Unicode characters**, depending on the dataset\n",
    "\n",
    "üõë **Note:**  \n",
    "If, during inference, a character not present in the training corpus is encountered (like an emoji or unseen character), it will be converted to the **unknown token** (`[UNK]`).\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ **3. Learning Merge Rules**\n",
    "\n",
    "- The goal is to gradually **merge the most frequent pairs of adjacent tokens**.\n",
    "- At each step:\n",
    "  1. Identify the **most frequent consecutive pair** of tokens in the corpus.\n",
    "  2. Merge that pair into a new token.\n",
    "  3. Add this new token to the vocabulary.\n",
    "\n",
    "üß† **Why?**  \n",
    "This allows the tokenizer to build more meaningful subwords over time, starting from characters ‚Üí two-character units ‚Üí full subwords.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ **4. Example ‚Äì Step-by-Step BPE Merge**\n",
    "\n",
    "**Initial Splits:**\n",
    "\n",
    "| Word    | Split Characters     | Frequency |\n",
    "|---------|----------------------|-----------|\n",
    "| \"hug\"   | [\"h\", \"u\", \"g\"]      | 10        |\n",
    "| \"pug\"   | [\"p\", \"u\", \"g\"]      | 5         |\n",
    "| \"pun\"   | [\"p\", \"u\", \"n\"]      | 12        |\n",
    "| \"bun\"   | [\"b\", \"u\", \"n\"]      | 4         |\n",
    "| \"hugs\"  | [\"h\", \"u\", \"g\", \"s\"] | 5         |\n",
    "\n",
    "**Count all adjacent pairs across corpus:**\n",
    "\n",
    "| Pair     | Frequency |\n",
    "|----------|-----------|\n",
    "| (\"u\", \"g\") | 20        |\n",
    "| (\"u\", \"n\") | 16        |\n",
    "| (\"h\", \"u\") | 15        |\n",
    "| (\"p\", \"u\") | 17        |\n",
    "| ...        | ...       |\n",
    "\n",
    "**Step 1:**  \n",
    "- Most frequent: `(\"u\", \"g\") ‚Üí \"ug\"`\n",
    "- Vocabulary after merge: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]`\n",
    "- Updated corpus:\n",
    "  ```\n",
    "  (\"h\", \"ug\"), (\"p\", \"ug\"), (\"p\", \"u\", \"n\"), (\"b\", \"u\", \"n\"), (\"h\", \"ug\", \"s\")\n",
    "  ```\n",
    "\n",
    "**Step 2:**  \n",
    "- Most frequent: `(\"u\", \"n\") ‚Üí \"un\"`\n",
    "- Vocabulary after merge: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]`\n",
    "- Updated corpus:\n",
    "  ```\n",
    "  (\"h\", \"ug\"), (\"p\", \"ug\"), (\"p\", \"un\"), (\"b\", \"un\"), (\"h\", \"ug\", \"s\")\n",
    "  ```\n",
    "\n",
    "**Step 3:**  \n",
    "- Most frequent: `(\"h\", \"ug\") ‚Üí \"hug\"`\n",
    "- Vocabulary after merge: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]`\n",
    "- Updated corpus:\n",
    "  ```\n",
    "  (\"hug\"), (\"p\", \"ug\"), (\"p\", \"un\"), (\"b\", \"un\"), (\"hug\", \"s\")\n",
    "  ```\n",
    "\n",
    "And so on, until the desired **vocabulary size** (e.g., 30,000 or 50,000 tokens) is reached.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **5. Inference / Tokenization Process**\n",
    "\n",
    "**Tokenization of new inputs follows these steps:**\n",
    "\n",
    "5.1. **Normalization**  \n",
    "   - Input text is cleaned (e.g., lowercased, Unicode normalized).\n",
    "\n",
    "5.2. **Pre-tokenization**  \n",
    "   - Text is split into words or symbols (e.g., using whitespace, punctuation).\n",
    "\n",
    "5.3. **Character Splitting**  \n",
    "   - Each word is split into its individual characters.  \n",
    "     Example: `\"bug\"` ‚Üí `[\"b\", \"u\", \"g\"]`\n",
    "\n",
    "5.4. **Apply Merge Rules**  \n",
    "   - Use the **learned merge rules** (from training) in order to combine frequent character pairs.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Example: Learned Merge Rules  \n",
    "Assume the following merge rules were learned during training:\n",
    "- `(\"u\", \"g\") ‚Üí \"ug\"`\n",
    "- `(\"u\", \"n\") ‚Üí \"un\"`\n",
    "- `(\"h\", \"ug\") ‚Üí \"hug\"`\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Tokenizing Examples\n",
    "\n",
    "- **\"bug\"**  \n",
    "  - Start: `[\"b\", \"u\", \"g\"]`  \n",
    "  - Apply: `\"u\" + \"g\" ‚Üí \"ug\"`  \n",
    "  - Result: `[\"b\", \"ug\"]`  \n",
    "  - ‚úÖ Both tokens are known ‚Üí Valid tokens\n",
    "\n",
    "- **\"mug\"**  \n",
    "  - Start: `[\"m\", \"u\", \"g\"]`  \n",
    "  - Apply: `\"u\" + \"g\" ‚Üí \"ug\"`  \n",
    "  - Result: `[\"m\", \"ug\"]`  \n",
    "  - ‚ùå \"m\" not in base vocabulary ‚Üí replaced with `[UNK]`  \n",
    "  - Final: `[\"[UNK]\", \"ug\"]`\n",
    "\n",
    "- **\"thug\"**  \n",
    "  - Start: `[\"t\", \"h\", \"u\", \"g\"]`  \n",
    "  - Apply: `\"u\" + \"g\" ‚Üí \"ug\"` ‚Üí `[\"t\", \"h\", \"ug\"]`  \n",
    "  - Then: `\"h\" + \"ug\" ‚Üí \"hug\"` ‚Üí `[\"t\", \"hug\"]`  \n",
    "  - ‚ùå \"t\" not in base vocabulary ‚Üí replaced with `[UNK]`  \n",
    "  - Final: `[\"[UNK]\", \"hug\"]`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6064027",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß† **What Is Byte-Level BPE (Used in GPT-2 and RoBERTa)?**\n",
    "\n",
    "### üìå The Problem:\n",
    "Traditional subword tokenizers (like WordPiece or standard BPE) treat **characters as Unicode symbols**. But this can cause problems:\n",
    "- Some characters (like emojis, special symbols, accented letters, rare scripts) may not be in the training corpus.\n",
    "- These characters end up as **`[UNK]` tokens**, which means the model can‚Äôt understand or use them meaningfully.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° The GPT-2 & RoBERTa Solution: Byte-Level BPE\n",
    "\n",
    "Instead of treating text as a sequence of **Unicode characters**, Byte-Level BPE treats it as a sequence of **raw bytes**.\n",
    "\n",
    "### üîç What does that mean?\n",
    "\n",
    "- Any text ‚Äî regardless of language or symbols ‚Äî can be converted to a sequence of **bytes** (integers from `0` to `255`).\n",
    "- Since a byte can represent any character, the **base vocabulary size is fixed at 256**, covering **all possible characters**, including:\n",
    "  - Emojis (üôÇ)\n",
    "  - Accented letters (√©, √±)\n",
    "  - Currency symbols (‚Çπ, ¬•)\n",
    "  - Control characters and whitespace (like `\\n`, `\\t`)\n",
    "  \n",
    "üõ°Ô∏è **Result:** No `[UNK]` tokens ‚Äî every possible character is representable.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Example:\n",
    "Let‚Äôs take the word:  \n",
    "```text\n",
    "\"caf√©‚òï\"\n",
    "```\n",
    "\n",
    "- Standard tokenizer might fail with `√©` or `‚òï` if not in vocab ‚Üí `[UNK]`\n",
    "- Byte-level BPE encodes each character into its **byte value**:\n",
    "  - `\"c\"` ‚Üí 99\n",
    "  - `\"a\"` ‚Üí 97\n",
    "  - `\"f\"` ‚Üí 102\n",
    "  - `\"√©\"` ‚Üí 195, 169 (multi-byte)\n",
    "  - `\"‚òï\"` ‚Üí 226, 152, 149 (multi-byte)\n",
    "  \n",
    "Then BPE learns merge rules **on byte sequences**, not characters.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ So What‚Äôs the BPE Part?\n",
    "\n",
    "Just like standard BPE:\n",
    "- Byte-level BPE starts with individual **bytes**.\n",
    "- It merges **frequent byte sequences** into longer tokens:\n",
    "  - `\"##ca\"`, `\"##f√©\"`, `\"##‚òï\"` might be common merges.\n",
    "  \n",
    "Eventually, the tokenizer might learn to represent `\"caf√©\"` as one token and `\"‚òï\"` as another.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Benefits of Byte-Level BPE:\n",
    "- **Robustness:** Can handle **any character** ‚Äî no need for `[UNK]`.\n",
    "- **Compact Base Vocabulary:** Only 256 initial tokens (the 256 byte values).\n",
    "- **Cross-language friendly:** Works for **multilingual and noisy text**.\n",
    "- **Precise recovery:** Can **reconstruct the original text exactly** from tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d23f13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99, 97, 102, 195, 169, 32, 226, 152, 149]\n"
     ]
    }
   ],
   "source": [
    "text = \"caf√© ‚òï\"\n",
    "byte_sequence = list(text.encode(\"utf-8\"))\n",
    "print(byte_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f12b71",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üí° What Are Bytes and Why Are They 0‚Äì255?\n",
    "\n",
    "### üî¢ 1. **What is a Byte?**\n",
    "- A **byte** is a unit of digital data that consists of **8 bits**.\n",
    "- Each **bit** is a binary digit: either `0` or `1`.\n",
    "- So, a byte is something like `01001101`, `11111111`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä 2. **How Many Values Can a Byte Represent?**\n",
    "\n",
    "Each bit can be:\n",
    "- `0` or `1` ‚Üí **2 possibilities**\n",
    "\n",
    "So, with 8 bits, the total number of unique combinations is:\n",
    "```\n",
    "2^8 = 256\n",
    "```\n",
    "\n",
    "That means:\n",
    "- A single byte can represent **256 distinct values**.\n",
    "- These values range from:\n",
    "  - `00000000` (in binary) = **0**\n",
    "  - to `11111111` (in binary) = **255**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç 3. **Why 0 to 255?**\n",
    "\n",
    "Because binary counting starts from 0:\n",
    "- `00000000` ‚Üí 0  \n",
    "- `00000001` ‚Üí 1  \n",
    "- `00000010` ‚Üí 2  \n",
    "- ...  \n",
    "- `11111111` ‚Üí 255  \n",
    "\n",
    "So all byte values fall in the range **[0, 255]**\n",
    "\n",
    "---\n",
    "\n",
    "### üåê 4. **Why is This Used in NLP Tokenizers Like GPT-2?**\n",
    "\n",
    "When you treat text as a sequence of **raw bytes**, you ensure:\n",
    "- Every character ‚Äî even emojis, accented letters, or symbols ‚Äî can be represented.\n",
    "- No special encoding needed ‚Äî it‚Äôs just numbers from 0 to 255.\n",
    "- This allows **robust and universal tokenization**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e0843",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî§ **What is Unicode?**\n",
    "\n",
    "- **Unicode** is a **standard** ‚Äî it defines a **universal set of characters** and assigns each character a **unique code point** (just a number).\n",
    "- Example characters and their Unicode code points:\n",
    "  - `\"A\"` ‚Üí U+0041\n",
    "  - `\"√©\"` ‚Üí U+00E9\n",
    "  - `\"‚òï\"` ‚Üí U+2615\n",
    "\n",
    "üí° **Unicode ‚â† Encoding**  \n",
    "Unicode only **assigns IDs to characters**, it does **not say how to store or transmit them** in bytes.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ **What is UTF-8 (and How is it Different)?**\n",
    "\n",
    "- **UTF-8** is an **encoding format** ‚Äî it tells the computer **how to store Unicode characters as bytes**.\n",
    "- UTF-8 is the most common encoding used on the web and in modern software.\n",
    "\n",
    "### üî¢ How UTF-8 Works:\n",
    "UTF-8 stores Unicode code points using **1 to 4 bytes**, depending on the character:\n",
    "\n",
    "| Unicode Range        | UTF-8 Byte Length |\n",
    "|----------------------|-------------------|\n",
    "| U+0000 to U+007F     | 1 byte (ASCII)    |\n",
    "| U+0080 to U+07FF     | 2 bytes           |\n",
    "| U+0800 to U+FFFF     | 3 bytes           |\n",
    "| U+10000 to U+10FFFF  | 4 bytes           |\n",
    "\n",
    "### üß™ Example:\n",
    "Let‚Äôs take the string: `\"caf√© ‚òï\"`\n",
    "\n",
    "| Character | Unicode Code Point | UTF-8 Bytes (decimal) | UTF-8 Bytes (hex) |\n",
    "|-----------|--------------------|------------------------|-------------------|\n",
    "| `c`       | U+0063             | [99]                   | 0x63              |\n",
    "| `a`       | U+0061             | [97]                   | 0x61              |\n",
    "| `f`       | U+0066             | [102]                  | 0x66              |\n",
    "| `√©`       | U+00E9             | [195, 169]             | 0xC3 0xA9         |\n",
    "| space     | U+0020             | [32]                   | 0x20              |\n",
    "| `‚òï`       | U+2615             | [226, 152, 149]        | 0xE2 0x98 0x95    |\n",
    "\n",
    "So the UTF-8 encoded byte sequence is:\n",
    "```python\n",
    "[99, 97, 102, 195, 169, 32, 226, 152, 149]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Why UTF-8 Is Useful**\n",
    "\n",
    "- **Backward compatible with ASCII** (first 128 characters use just 1 byte)\n",
    "- **Compact for English text**, while still supporting all of Unicode\n",
    "- Handles **all global scripts**, emojis, and symbols\n",
    "- Avoids `[UNK]` in byte-level tokenization (like in GPT-2)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29b9c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî§ WordPiece Tokenization Algorithm \n",
    "\n",
    "---\n",
    "\n",
    "(HuggingFace Notebook: https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb)\n",
    "\n",
    "### üõ†Ô∏è **Training Phase**\n",
    "\n",
    "- **1. Normalization**\n",
    "  - Clean the training corpus text.\n",
    "  - Examples: convert to lowercase, strip accents, normalize Unicode, etc.\n",
    "\n",
    "- **2. Pre-tokenization**\n",
    "  - Split the normalized text into words (usually using whitespace and punctuation).\n",
    "  - Example:  \n",
    "    ```\n",
    "    \"hugging is fun\" ‚Üí [\"hugging\", \"is\", \"fun\"]\n",
    "    ```\n",
    "\n",
    "- **3. Initial Vocabulary Construction**\n",
    "  - Start with:\n",
    "    - A few **special tokens** (like `[CLS]`, `[SEP]`, `[UNK]`, etc.)\n",
    "    - All **unique characters** seen in the training corpus.\n",
    "  - Characters are treated differently depending on their position:\n",
    "    - **First character** of a word is kept as-is.\n",
    "    - **Subsequent characters** are prefixed with `##`.\n",
    "  - Example for word `\"word\"`:\n",
    "    ```\n",
    "    Split as: [\"w\", \"##o\", \"##r\", \"##d\"]\n",
    "    ```\n",
    "  - This creates a vocabulary like:\n",
    "    ```\n",
    "    [\"w\", \"##o\", \"##r\", \"##d\", ...]\n",
    "    ```\n",
    "\n",
    "- **4. Corpus Representation (Example)**  \n",
    "  Let‚Äôs say our corpus is:\n",
    "  ```\n",
    "  (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "  ```\n",
    "  We represent them as:\n",
    "  - `\"hug\"` ‚Üí [\"h\", \"##u\", \"##g\"]\n",
    "  - `\"pug\"` ‚Üí [\"p\", \"##u\", \"##g\"]\n",
    "  - `\"pun\"` ‚Üí [\"p\", \"##u\", \"##n\"]\n",
    "  - `\"bun\"` ‚Üí [\"b\", \"##u\", \"##n\"]\n",
    "  - `\"hugs\"` ‚Üí [\"h\", \"##u\", \"##g\", \"##s\"]\n",
    "\n",
    "- **5. Merge Rule Selection (Scoring)**  \n",
    "  - WordPiece selects which pairs to merge based on this score:\n",
    "    \\[\n",
    "    \\text{score} = \\frac{\\text{frequency of pair}}{\\text{freq of first element} \\times \\text{freq of second element}}\n",
    "    \\]\n",
    "  - This helps prioritize **rare parts** merging over just frequent ones.\n",
    "  - Example:\n",
    "    - Even though `(\"##u\", \"##g\")` is common, `\"##g\"` + `\"##s\"` may be merged first due to a higher score.\n",
    "\n",
    "- **6. Vocabulary Growth**  \n",
    "  - Merge pairs iteratively until the desired vocabulary size is reached.\n",
    "  - Each merge adds a new token to the vocabulary and updates the corpus.\n",
    "  - Example merge steps:\n",
    "    1. `(\"##g\", \"##s\")` ‚Üí `\"##gs\"`\n",
    "    2. `(\"h\", \"##u\")` ‚Üí `\"hu\"`\n",
    "    3. `(\"hu\", \"##g\")` ‚Üí `\"hug\"`\n",
    "    4. ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53253b1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß© Tokenization Algorithm (WordPiece vs BPE)\n",
    "\n",
    "### üöÄ WordPiece Tokenization Logic\n",
    "\n",
    "- **WordPiece does not store merge rules**, only the **final vocabulary** built during training.\n",
    "- Tokenization is based on a **greedy longest-match-first strategy**:\n",
    "  - At each step, find the **longest substring (subword)** from the current position that exists in the vocabulary.\n",
    "  - If no such subword exists, the entire word is marked as `[UNK]`.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Example 1: Tokenizing `\"hugs\"` (WordPiece)\n",
    "\n",
    "- Assume `\"hug\"` and `\"##s\"` are in the vocabulary.\n",
    "- Longest match from beginning: `\"hug\"` ‚Üí split\n",
    "- Remaining: `\"##s\"` ‚Üí also in vocabulary\n",
    "- ‚úÖ Final tokenization: `[\"hug\", \"##s\"]`\n",
    "\n",
    "#### üîÅ Comparison with BPE:\n",
    "- BPE would apply merges in order, possibly resulting in:  \n",
    "  `[\"hu\", \"##gs\"]`  \n",
    "- So **the token sequence is different** between WordPiece and BPE.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Example 2: Tokenizing `\"bugs\"`\n",
    "\n",
    "- Step-by-step:\n",
    "  1. `\"b\"` is the longest match ‚Üí keep `\"b\"`, remaining is `\"##ugs\"`\n",
    "  2. `\"##u\"` is the longest subword match in `\"##ugs\"` ‚Üí keep `\"##u\"`, remaining is `\"##gs\"`\n",
    "  3. `\"##gs\"` is in the vocabulary ‚Üí keep it\n",
    "- ‚úÖ Final tokenization: `[\"b\", \"##u\", \"##gs\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Example 3: Tokenizing `\"mug\"`\n",
    "\n",
    "- `\"m\"` might be in the initial alphabet, but `\"##u\"` and `\"##g\"` need to be in the vocabulary too.\n",
    "- Let‚Äôs assume `\"##g\"` is not in vocabulary:\n",
    "  - Tokenization fails ‚Üí no valid split found\n",
    "- ‚ùå Final tokenization: `[\"[UNK]\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Example 4: Tokenizing `\"bum\"`\n",
    "\n",
    "- Even though `\"b\"` and `\"##u\"` might be in vocabulary,\n",
    "  - `\"##m\"` is **not** in vocabulary\n",
    "- So, tokenization is **not partial** ‚Äî **entire word is marked as unknown**\n",
    "- ‚ùå Final tokenization: `[\"[UNK]\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Try This: Tokenize `\"pugs\"` (WordPiece)\n",
    "\n",
    "Let‚Äôs say the following are in vocabulary:\n",
    "- `\"p\"`, `\"##u\"`, `\"##g\"`, `\"##s\"`, `\"##gs\"`, and `\"pug\"`\n",
    "\n",
    "**Tokenization steps:**\n",
    "1. `\"pug\"` is the longest valid match from beginning ‚Üí keep `\"pug\"`\n",
    "2. Remaining: `\"##s\"` ‚Üí in vocabulary\n",
    "3. ‚úÖ Final tokenization: `[\"pug\", \"##s\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e1fba",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Advantages of WordPiece over BPE\n",
    "\n",
    "### 1. **Better Handling of Subword Composition**\n",
    "- **WordPiece uses a scoring function** (based on mutual information) instead of just frequency.\n",
    "  \\[\n",
    "  \\text{score} = \\frac{\\text{freq(pair)}}{\\text{freq(left)} \\times \\text{freq(right)}}\n",
    "  \\]\n",
    "- This **penalizes common subwords** and favors merges that create **meaningful units** (e.g., medical terms, rare suffixes).\n",
    "- üü¢ **Result**: WordPiece builds more semantically useful subwords.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Lower Risk of Ambiguity in Frequent Subword Reuse**\n",
    "- BPE may repeatedly merge frequent chunks (like `##ing`, `##tion`) even when they‚Äôre not optimal.\n",
    "- WordPiece avoids merging overly common units just because they appear often, reducing **ambiguity and overfitting to common patterns**.\n",
    "- WordPiece penalizes merging frequent parts unless the pair appears more often together than you'd expect by chance. Since uses score instead of frequency\n",
    "---\n",
    "\n",
    "### 3. **More Controlled Vocabulary Growth**\n",
    "- Because WordPiece merges based on information content, it often forms **more diverse and useful subwords** within the same vocab size.\n",
    "- üü¢ Leads to better **vocabulary efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **More Consistent and Compact Tokens**\n",
    "- In many cases, WordPiece produces **fewer tokens** per word for morphologically complex or rare words.\n",
    "- This results in **shorter sequences**, which is beneficial for:\n",
    "  - **Training speed**\n",
    "  - **Memory efficiency**\n",
    "  - **Model performance** (e.g., in attention mechanisms)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Better Performance on Downstream Tasks**\n",
    "- Empirically, models using WordPiece (like **BERT**) often achieve **higher accuracy** on tasks like NER, sentiment analysis, and QA.\n",
    "- While this is partly due to BERT's architecture, WordPiece contributes by giving **cleaner, more meaningful token splits**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e4f33",
   "metadata": {},
   "source": [
    "\n",
    "## üî§ Unigram Tokenization Algorithm\n",
    "\n",
    "HuggingFace Link: (https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section7.ipynb)\n",
    "### üß† Core Idea:\n",
    "- Unlike **BPE** and **WordPiece** which build vocabulary by merging subwords,\n",
    "- **Unigram** starts with a **large vocabulary** of possible subwords and **removes** the least useful tokens until it reaches a desired size.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Training Phase\n",
    "\n",
    "#### 1. **Initial Vocabulary Creation**\n",
    "- Start with a **very large vocabulary** of candidate subwords.\n",
    "- These subwords can be:\n",
    "  - All **strict substrings** of the corpus words.\n",
    "  - Or generated using BPE with a **very large** initial vocab size.\n",
    "\n",
    "##### üîé Example Corpus:\n",
    "```text\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "\n",
    "##### üéí Initial Vocabulary (All substrings):\n",
    "```\n",
    "[\"h\", \"u\", \"g\", \"hu\", \"ug\", \"hug\", \n",
    " \"p\", \"pu\", \"pug\", \n",
    " \"n\", \"un\", \"pun\", \n",
    " \"b\", \"bu\", \"bun\", \n",
    " \"s\", \"gs\", \"hugs\", ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Tokenization & Loss Computation**\n",
    "- For each word, the algorithm tries to **tokenize it in all possible ways** using the current vocabulary.\n",
    "- It assigns a **probability** to each token and computes the **likelihood (loss)** of the corpus under the current vocab.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Prune Vocabulary Iteratively**\n",
    "- For each token, compute how much the **total loss would increase** if we removed that token.\n",
    "- Tokens that **increase the loss the least** are considered **least important**.\n",
    "- Instead of removing one token at a time (slow!), remove the **lowest `p%`** of tokens (e.g., 10‚Äì20%) in each iteration.\n",
    "- üîÅ **Repeat** until the desired vocabulary size is reached.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Preserve Base Characters**\n",
    "- To ensure any word can be tokenized, the **base characters (a‚Äìz, digits, etc.) are never removed.**\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Key Features of Unigram Tokenization\n",
    "\n",
    "| Feature | Unigram |\n",
    "|---------|---------|\n",
    "| Direction | Starts from large vocab ‚Üí prunes down |\n",
    "| Core Mechanism | Removes least important tokens by measuring loss impact |\n",
    "| Probabilistic? | ‚úÖ Yes ‚Äî considers all valid segmentations |\n",
    "| Final Vocabulary | Subwords that contribute most to corpus likelihood |\n",
    "| Unknown Token Handling | Rare (base chars always kept) |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages Over BPE/WordPiece\n",
    "\n",
    "- Allows **multiple valid tokenizations** of a word and picks the best.\n",
    "- Produces **more compact and semantically meaningful** token sets.\n",
    "- **More flexible** during decoding and post-processing.\n",
    "- Often used in **SentencePiece** (used by models like T5, XLNet).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d18a62",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üî§ Unigram Language Model Tokenization\n",
    "\n",
    "---\n",
    "\n",
    "### üìò What is a Unigram Language Model?\n",
    "\n",
    "- A **Unigram Language Model** treats each token as **independent** of the previous tokens.\n",
    "- That means:\n",
    "  \\[\n",
    "  P(w_1, w_2, ..., w_n) = P(w_1) \\times P(w_2) \\times ... \\times P(w_n)\n",
    "  \\]\n",
    "- If we **generated** text using this model, it would always produce the **most frequent token**, ignoring context ‚Äî hence, it‚Äôs the simplest possible language model.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Vocabulary & Token Probabilities\n",
    "\n",
    "- In Unigram tokenization, each **subword** in the vocabulary is assigned a probability based on its **frequency in the corpus**:\n",
    "  \\[\n",
    "  P(token) = \\frac{\\text{frequency of token}}{\\text{sum of frequencies of all tokens}}\n",
    "  \\]\n",
    "\n",
    "#### üîé Example Corpus:\n",
    "```text\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "\n",
    "#### üì¶ Subword Frequencies (from all words):\n",
    "```\n",
    "(\"h\", 15)   (\"u\", 36)   (\"g\", 20)\n",
    "(\"hu\", 15)  (\"ug\", 20)  (\"hug\", 15)\n",
    "(\"p\", 17)   (\"pu\", 17)  (\"pug\", 5)\n",
    "(\"n\", 16)   (\"un\", 16)  (\"pun\", 12)\n",
    "(\"b\", 4)    (\"bu\", 4)   (\"bun\", 4)\n",
    "(\"s\", 5)    (\"gs\", 5)   (\"ugs\", 5)\n",
    "```\n",
    "\n",
    "- üî¢ **Total frequency sum** = **210**\n",
    "- Example:  \n",
    "  \\[\n",
    "  P(\"ug\") = \\frac{20}{210} \\approx 0.0952\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÇÔ∏è Tokenizing a Word: Try All Possible Segmentations\n",
    "\n",
    "To tokenize a word:\n",
    "- Try **all possible ways** of splitting it using valid subwords.\n",
    "- Compute the **probability** of each segmentation by multiplying the probabilities of individual subwords.\n",
    "\n",
    "#### üìå Example: Tokenizing `\"pug\"`\n",
    "\n",
    "**Option 1:** `[\"p\", \"u\", \"g\"]`\n",
    "\\[\n",
    "P = \\frac{17}{210} \\times \\frac{36}{210} \\times \\frac{20}{210} \\approx 0.000389\n",
    "\\]\n",
    "\n",
    "**Option 2:** `[\"pu\", \"g\"]`\n",
    "\\[\n",
    "P = \\frac{17}{210} \\times \\frac{20}{210} \\approx 0.0022676\n",
    "\\]\n",
    "\n",
    "**Option 3:** `[\"p\", \"ug\"]`\n",
    "\\[\n",
    "P = \\frac{17}{210} \\times \\frac{20}{210} \\approx 0.0022676\n",
    "\\]\n",
    "\n",
    "üü¢ **Best Tokenization**: Either `[\"p\", \"ug\"]` or `[\"pu\", \"g\"]` (equal score in this case)\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why Shorter Tokenizations Are Preferred\n",
    "\n",
    "- Each additional token means multiplying by an extra probability (less than 1).\n",
    "- So **fewer tokens ‚Üí higher total probability**.\n",
    "- This matches our intuition: **split into as few tokens as possible**.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ Efficient Tokenization with Viterbi Algorithm\n",
    "\n",
    "- Trying all segmentations becomes slow for long words.\n",
    "- So we use the **Viterbi algorithm** ‚Äî a dynamic programming approach.\n",
    "\n",
    "#### üß© How Viterbi Works:\n",
    "\n",
    "1. For each character position in the word, store the **best segmentation** ending there.\n",
    "2. For each possible subword ending at a position, look back at the best segmentation that ends at the start of that subword.\n",
    "3. Multiply their probabilities to find the best path.\n",
    "4. After reaching the last character, **backtrack** to recover the best segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Example: Tokenizing `\"unhug\"` using Viterbi\n",
    "\n",
    "Given subword probabilities:\n",
    "- `\"u\"`: 0.1714\n",
    "- `\"un\"`: 0.0762\n",
    "- `\"h\"`: 0.0714\n",
    "- `\"hu\"`: 0.0714\n",
    "- `\"hug\"`: 0.0714\n",
    "\n",
    "**Step-by-step Segmentation & Scores:**\n",
    "\n",
    "| Position | Best Subword Ending | Score           | Full Segmentation       |\n",
    "|----------|---------------------|------------------|--------------------------|\n",
    "| 0 (u)    | `\"u\"`               | 0.1714           | `[\"u\"]`                  |\n",
    "| 1 (n)    | `\"un\"`              | 0.0762           | `[\"un\"]`                 |\n",
    "| 2 (h)    | `\"h\"`               | `0.0762 √ó 0.0714 ‚âà 0.0054` | `[\"un\", \"h\"]`      |\n",
    "| 3 (u)    | `\"hu\"`              | `0.0762 √ó 0.0714 ‚âà 0.0054` | `[\"un\", \"hu\"]`     |\n",
    "| 4 (g)    | `\"hug\"`             | `0.0762 √ó 0.0714 ‚âà 0.0054` | ‚úÖ `[\"un\", \"hug\"]` |\n",
    "\n",
    "üü¢ Final Tokenization: `[\"un\", \"hug\"]` (highest score path)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary: Why Use Unigram?\n",
    "\n",
    "| Feature                            | Unigram Tokenizer                          |\n",
    "|-----------------------------------|--------------------------------------------|\n",
    "| Training Direction                | Start with large vocab ‚Üí prune down        |\n",
    "| Probabilistic Tokenization        | ‚úÖ Yes                                      |\n",
    "| Token Scoring                     | Based on token frequencies in corpus       |\n",
    "| Preferred Segmentation            | Shortest (fewest tokens = highest score)   |\n",
    "| Algorithm for Efficiency          | Viterbi algorithm                          |\n",
    "| Unknown Token Handling            | Base characters are always kept (fallback) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4238648",
   "metadata": {},
   "source": [
    "---\n",
    "## HuggingFace Link for Building Tokenizer Block by Block: (https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section8.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
