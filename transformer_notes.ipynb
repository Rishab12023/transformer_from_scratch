{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537b6696",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🧠 **WordPiece Tokenizer — Summary**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔷 What is WordPiece Tokenizer?\n",
    "\n",
    "**WordPiece** is a **subword-level tokenization algorithm** used in models like **BERT**, **DistilBERT**, and **ALBERT**.  \n",
    "It breaks words into smaller known units (called subwords) to effectively handle:\n",
    "- Rare words\n",
    "- Unseen words\n",
    "- Large vocabulary issues\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Why Use WordPiece?\n",
    "\n",
    "| Problem                   | Solution via WordPiece |\n",
    "|--------------------------|------------------------|\n",
    "| Out-of-vocabulary words  | Break into known subwords |\n",
    "| Large vocab size         | Use smaller subword vocab |\n",
    "| Domain-specific words    | Represent via common sub-parts (e.g., \"bio\"+\"chemistry\") |\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ How is WordPiece Tokenizer **Trained**?\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. **Prepare Corpus**  \n",
    "   - Input text corpus split into words using **whitespace** as delimiter.\n",
    "\n",
    "   Example sentence:  \n",
    "   ```\n",
    "   The quick brown foxes jumped over the lazy dog.\n",
    "   ```\n",
    "\n",
    "   Split into words:\n",
    "   ```\n",
    "   [\"The\", \"quick\", \"brown\", \"foxes\", \"jumped\", ...]\n",
    "   ```\n",
    "\n",
    "2. **Initialize Vocabulary**  \n",
    "   - Start with:\n",
    "     - All **unique characters**\n",
    "     - Special tokens (`[CLS]`, `[SEP]`, `[UNK]`, etc.)\n",
    "\n",
    "3. **Iterative Merging**  \n",
    "   - Count **frequent adjacent symbol pairs** in all words.\n",
    "   - Merge the most frequent pair into a new token.\n",
    "   - Add that to the vocabulary.\n",
    "   - Repeat until vocab reaches a desired size (e.g., 30,000 tokens).\n",
    "\n",
    "   Example word `\"unhappiness\"` might evolve like:\n",
    "   ```\n",
    "   [\"u\", \"n\", \"h\", \"a\", \"p\", \"p\", \"i\", \"n\", \"e\", \"s\", \"s\"]\n",
    "   → \"p\"+\"p\" → \"pp\"\n",
    "   → \"h\"+\"a\" → \"ha\"\n",
    "   → \"ha\"+\"pp\" → \"happ\"\n",
    "   → \"happ\"+\"i\" → \"happi\"\n",
    "   → ...\n",
    "   ```\n",
    "\n",
    "   Resulting vocab may contain: `[\"un\", \"happi\", \"ness\", \"happy\"]`\n",
    "\n",
    "4. **Save Vocab**  \n",
    "   - Save the final vocabulary to a `vocab.txt` file (used by models like BERT).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔤 How Does WordPiece Tokenize Text?\n",
    "\n",
    "- Uses **greedy longest-prefix matching** to tokenize a word.\n",
    "- If a subword is not at the beginning of a word, it is prefixed with `##`.\n",
    "\n",
    "### Example: `\"unhappiness\"`\n",
    "\n",
    "If vocab contains:\n",
    "```\n",
    "[\"un\", \"happi\", \"ness\"]\n",
    "```\n",
    "\n",
    "Tokenized output:\n",
    "```\n",
    "[\"un\", \"##happi\", \"##ness\"]\n",
    "```\n",
    "\n",
    "- `\"un\"` is a valid word-start token.\n",
    "- `\"##happi\"` and `\"##ness\"` are continuation subwords.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Properties of WordPiece\n",
    "\n",
    "| Feature                       | Details |\n",
    "|------------------------------|---------|\n",
    "| Token granularity            | Subword |\n",
    "| Initial splitting            | Based on **whitespace** |\n",
    "| Continuation token prefix    | `##` |\n",
    "| Vocab learned from data?     | ✅ Yes, using frequency |\n",
    "| Handles OOV words?           | ✅ Yes, splits into smaller parts |\n",
    "| Fixed vocab during fine-tuning? | ✅ Yes |\n",
    "| Commonly used in             | BERT, DistilBERT, ALBERT |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Limitations\n",
    "\n",
    "| Limitation | Details |\n",
    "|------------|---------|\n",
    "| Greedy matching | Doesn’t always find optimal splits |\n",
    "| Not suitable for languages without spaces | Like Chinese, Japanese — use SentencePiece instead |\n",
    "| No dynamic vocab growth | Can't learn new subwords during fine-tuning |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Real Example in Code\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.tokenize(\"unhappiness\"))  # ['un', '##happi', '##ness']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a visual diagram to show how WordPiece training works step-by-step? Or want to train one yourself from a text file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdf1a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d72a5b57",
   "metadata": {},
   "source": [
    "That’s a **fantastic question** — and your intuition is spot-on for digging deeper. Let's clear the confusion and fully explain:\n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 What is the **Main Objective** of a Tokenizer in NLP?\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 1. **Bridge Between Text and Numbers**\n",
    "\n",
    "The first and foremost job of any tokenizer is to:\n",
    "> ✅ **Convert raw text into a sequence of tokens (numbers)** that can be fed into machine learning models.\n",
    "\n",
    "Because models can’t understand:\n",
    "```\n",
    "\"The cat sat on the mat.\"\n",
    "```\n",
    "\n",
    "But they **can** understand:\n",
    "```\n",
    "[101, 1996, 4937, 2938, 2006, 1996, 8827, 102]  ← (example from BERT)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 2. **Handle Vocabulary Efficiently**\n",
    "\n",
    "Words are **open-ended** — new ones are created all the time:\n",
    "- slang: \"yeet\", \"finsta\"\n",
    "- domain-specific: \"viscosity\", \"LiDAR\", \"transformer-based\"\n",
    "\n",
    "If you only have a **fixed-word vocabulary** (word-level), you'd get many `[UNK]` (unknowns).\n",
    "\n",
    "Tokenizers like **WordPiece** or **BPE** help by:\n",
    "> ✅ **Breaking rare/unseen words into smaller, known subwords**, so you don’t need to rely on `[UNK]`.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Word        | Word-level Tokenizer | Subword Tokenizer (WordPiece/BPE) |\n",
    "|-------------|----------------------|-----------------------------------|\n",
    "| \"unhappiness\" | `[UNK]`               | `[\"un\", \"##happi\", \"##ness\"]`      |\n",
    "\n",
    "This improves:\n",
    "- Generalization\n",
    "- Vocabulary coverage\n",
    "- Performance on unseen data\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 3. **Capture Meaningful Units**\n",
    "\n",
    "Another goal:\n",
    "> ✅ Tokenizers aim to **capture semantically meaningful chunks** of language.\n",
    "\n",
    "For example:\n",
    "- `\"un\"`, `\"happy\"`, `\"ness\"` have meanings\n",
    "- Tokenizing `\"unhappiness\"` → `[\"un\", \"happy\", \"ness\"]` helps the model **understand the structure** of the word.\n",
    "\n",
    "This is better than treating each character individually:\n",
    "```\n",
    "[\"u\", \"n\", \"h\", \"a\", \"p\", \"p\", \"i\", \"n\", \"e\", \"s\", \"s\"]\n",
    "```\n",
    "— which would be **too fine-grained and less interpretable**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 4. **Minimize Sequence Length**\n",
    "\n",
    "Longer token sequences = slower training + more memory\n",
    "\n",
    "So tokenizers aim to:\n",
    "> ✅ **Pack as much information into as few tokens as possible**\n",
    "\n",
    "This is where subword tokenization wins:\n",
    "- More compact than character-level\n",
    "- More flexible than word-level\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 5. **Preserve Language Structure (syntax, morphology)**\n",
    "\n",
    "Good tokenizers preserve:\n",
    "- **Word boundaries**\n",
    "- **Subword patterns** (e.g., suffixes, prefixes)\n",
    "- **Punctuation and casing** when needed\n",
    "\n",
    "This helps models:\n",
    "- Understand grammar\n",
    "- Learn attention across words/subwords\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 6. **Enable Multilingual, Domain-Specific, and Robust NLP**\n",
    "\n",
    "In multilingual models like **XLM-R** or **mBERT**, or domain-specific models like **BioBERT**, tokenizers must:\n",
    "> ✅ **Handle multiple languages or technical jargon without retraining the whole model**\n",
    "\n",
    "They do this by using subword units that work **across languages** or domains.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary: Goals of a Tokenizer\n",
    "\n",
    "| Goal # | Objective |\n",
    "|--------|-----------|\n",
    "| 1️⃣ | Convert text to model-friendly format (numbers) |\n",
    "| 2️⃣ | Avoid unknown words (OOV) by using subwords |\n",
    "| 3️⃣ | Capture meaningful language chunks (morphemes) |\n",
    "| 4️⃣ | Minimize sequence length and computation |\n",
    "| 5️⃣ | Preserve language structure (syntax, case, space) |\n",
    "| 6️⃣ | Support multilingual and domain-specific text |\n",
    "| 7️⃣ | Be efficient and fast for inference + training |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Analogy: Tokenizer is like a **linguistically-aware zipper**\n",
    "\n",
    "- It zips together the raw mess of text into efficient, compressed, and model-digestible pieces.\n",
    "- Good tokenization = better understanding, fewer errors, faster models.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 In a Nutshell:\n",
    "> A tokenizer is **not just about OOV handling**, but about **efficiency, structure, generalization, and linguistic intelligence.**\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like:\n",
    "- A visual diagram comparing character/word/subword tokenization\n",
    "- A quick experiment to show how bad tokenization leads to model errors\n",
    "- Or guidance on picking the best tokenizer for your task/domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b971b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e937d4c5",
   "metadata": {},
   "source": [
    "You're thinking in exactly the right direction. Let’s dive deep into:\n",
    "\n",
    "1. 🔁 **Why finding most frequent adjacent pairs seems expensive**  \n",
    "2. 🧠 **Does it use recursion or a smarter algorithm?**  \n",
    "3. 📦 **What does \"training a tokenizer\" actually mean?**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 1. Is finding the most frequent adjacent pair expensive?\n",
    "\n",
    "Yes — **naively**, it is.\n",
    "\n",
    "For every iteration:\n",
    "- You need to scan the entire corpus.\n",
    "- For each word, check all adjacent token pairs.\n",
    "- Count how many times each pair occurs.\n",
    "- Merge the most frequent one.\n",
    "\n",
    "### ⛔ Naive time complexity:\n",
    "If you do this in a dumb way, time complexity is roughly:\n",
    "```\n",
    "O(V * L * M)\n",
    "```\n",
    "Where:\n",
    "- V = number of words\n",
    "- L = average length of a word\n",
    "- M = number of merge iterations\n",
    "\n",
    "That **adds up fast**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 2. How do real implementations make it fast?\n",
    "\n",
    "Instead of recursion, most implementations use:\n",
    "> ✅ **Efficient data structures** like **heaps**, **hash maps**, and **linked lists** to update pair frequencies incrementally.\n",
    "\n",
    "### 🔧 Core techniques:\n",
    "\n",
    "#### a) **Hash Map of Pair Counts**\n",
    "\n",
    "```python\n",
    "pair_counts = {\n",
    "  (\"h\", \"a\"): 412,\n",
    "  (\"a\", \"p\"): 590,\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "This allows **O(1)** updates and lookups.\n",
    "\n",
    "#### b) **Priority Queue (Min/Max Heap)**\n",
    "\n",
    "To always get the **most frequent pair efficiently** (like in Huffman Coding), use a max-heap:\n",
    "\n",
    "```python\n",
    "heap = [(-590, (\"a\", \"p\")), (-412, (\"h\", \"a\")), ...]\n",
    "```\n",
    "\n",
    "- Extract the top of the heap each time\n",
    "- Update only the affected pairs (not whole corpus!)\n",
    "\n",
    "#### c) **Linked Lists for Word Sequences**\n",
    "\n",
    "Each word can be stored as a linked list of tokens, so:\n",
    "- You don’t recreate the entire word each time\n",
    "- You just adjust pointers when a pair is merged\n",
    "\n",
    "### ✅ Result:\n",
    "> Instead of recalculating pair counts globally every time, you only update **locally** where changes occurred.\n",
    "\n",
    "This turns the operation from expensive to scalable (especially in libraries like Hugging Face's `tokenizers` or Google's `SentencePiece`).\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 3. What does **\"training a tokenizer\"** actually mean?\n",
    "\n",
    "When we say:\n",
    "> \"I trained a tokenizer on a corpus\"\n",
    "\n",
    "We mean:\n",
    "\n",
    "### ✅ You did the following:\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1️⃣ | Take a raw text corpus (large file) |\n",
    "| 2️⃣ | Pre-tokenize (split on whitespace, punctuation etc.) |\n",
    "| 3️⃣ | Initialize vocabulary (characters or bytes) |\n",
    "| 4️⃣ | Run **merge operations** (WordPiece/BPE/Unigram) |\n",
    "| 5️⃣ | Build a final vocabulary file (e.g., `vocab.txt`) |\n",
    "| 6️⃣ | Save tokenizer config (e.g., special tokens, pre-tokenization rules, tokenizer.json) |\n",
    "\n",
    "So the \"training\" of a tokenizer:\n",
    "> = Building a **vocabulary + merge rules** that define how to convert new text into token sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Tokenizer Training ≠ Model Training\n",
    "\n",
    "| Aspect              | Tokenizer Training        | Model Training              |\n",
    "|---------------------|---------------------------|-----------------------------|\n",
    "| Input               | Raw text                  | Tokenized input + labels    |\n",
    "| Output              | Vocabulary + token rules  | Learned weights             |\n",
    "| Objective           | Minimize token count / maximize subword coverage | Minimize loss function       |\n",
    "| Algorithm used      | BPE, WordPiece, Unigram LM| SGD, Adam, Backprop         |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Code Snippet to Train BPE Tokenizer\n",
    "\n",
    "```python\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(vocab_size=30000)\n",
    "\n",
    "# Train on your corpus\n",
    "tokenizer.train([\"corpus.txt\"], trainer)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "```\n",
    "\n",
    "You now have a trained tokenizer 🎉\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final Summary\n",
    "\n",
    "| Concept | Answer |\n",
    "|--------|--------|\n",
    "| Is pair counting expensive? | Yes naively, but optimized using heaps & hash maps |\n",
    "| Does it use recursion? | No — uses **incremental updates** and smart data structures |\n",
    "| What is tokenizer training? | Building a vocab + merge rules from corpus |\n",
    "| What is the output? | A tokenizer file with vocab + merge rules, used for inference |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you’d like to walk through **a small tokenizer training simulation** — we can try a tiny corpus and trace all merges manually!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8fda8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8304f98a",
   "metadata": {},
   "source": [
    "Absolutely! You're now digging into the **real mathematical backbone** of WordPiece. And yes — what you're referring to is the **scoring mechanism** used to **choose which pair to merge next**, which is **more nuanced** than just frequency-based BPE.\n",
    "\n",
    "Let’s walk through this carefully.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 WordPiece Scoring Function\n",
    "\n",
    "The score for a candidate merge pair `(A, B)` is given by:\n",
    "\n",
    "\\[\n",
    "\\text{score}(A, B) = \\frac{\\text{freq}(A \\ B)}{\\text{freq}(A) \\times \\text{freq}(B)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- `freq(A B)` = number of times the pair `A, B` occurs adjacently in the corpus\n",
    "- `freq(A)` = number of times `A` occurs (anywhere in corpus)\n",
    "- `freq(B)` = number of times `B` occurs (anywhere in corpus)\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why use this scoring function?\n",
    "\n",
    "This is inspired by **Pointwise Mutual Information (PMI)** from information theory.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Intuition\n",
    "\n",
    "If two subwords `A` and `B` **occur often together**, but **rarely occur individually**, then:\n",
    "- `freq(A B)` is relatively high\n",
    "- `freq(A)` and `freq(B)` are low\n",
    "- So the score becomes **high**\n",
    "- → Meaning `A` and `B` form a **strong collocation** (likely to form a new meaningful token)\n",
    "\n",
    "Whereas:\n",
    "- If `A` and `B` are **individually common** (like `\"un\"` or `\"##ing\"`), but appear in many contexts, then:\n",
    "  - Their individual frequencies are high\n",
    "  - Even if `freq(A B)` is high, the **score is lower**\n",
    "  - → Less incentive to merge them, because they’re too \"generic\"\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 So what does this scoring function do?\n",
    "\n",
    "It helps identify subword pairs that:\n",
    "- Appear **often together**\n",
    "- But are **not frequent alone**\n",
    "\n",
    "Which means: it finds **subwords that are tightly bound** and likely to represent **a single semantic unit** (like `\"hu\" + \"##gging\"` → `\"hugging\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 Compared to BPE\n",
    "\n",
    "| Metric              | BPE                                 | WordPiece                           |\n",
    "|---------------------|--------------------------------------|--------------------------------------|\n",
    "| Merge logic         | Most frequent adjacent pair          | Pair with highest normalized score   |\n",
    "| Score formula       | Just frequency                      | `freq(pair) / (freq(A) * freq(B))`   |\n",
    "| Effect              | Merges common pairs (may be generic) | Merges **specific** and meaningful units |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Relation to Probabilities\n",
    "\n",
    "While not **exactly** a probability, this score is related to **PMI**, which is computed as:\n",
    "\n",
    "\\[\n",
    "\\text{PMI}(A, B) = \\log \\frac{P(A, B)}{P(A) \\cdot P(B)}\n",
    "\\]\n",
    "\n",
    "Now, using raw counts instead of probabilities (and skipping the `log`), the WordPiece score mimics:\n",
    "\n",
    "\\[\n",
    "\\text{score}(A, B) = \\frac{\\text{freq}(A, B)}{\\text{freq}(A) \\cdot \\text{freq}(B)}\n",
    "\\]\n",
    "\n",
    "So it’s **proportional to an unnormalized joint probability**, which:\n",
    "- Prioritizes **semantic closeness** between A and B\n",
    "- Helps build a vocabulary with **highly specific and tight subword units**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Example: `\"hugging\"` vs `\"un + able\"`\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "| Pair           | freq(pair) | freq(A) | freq(B) | score           |\n",
    "|----------------|------------|---------|---------|------------------|\n",
    "| `\"hu\"`, `\"##gging\"` | 100        | 120     | 130     | 100 / (120 × 130) ≈ **0.0064** |\n",
    "| `\"un\"`, `\"##able\"`  | 300        | 500     | 400     | 300 / (500 × 400) = **0.0015** |\n",
    "\n",
    "Even though `\"un\"+\"##able\"` is more frequent, `\"hu\"+\"##gging\"` has a **higher score** and is therefore more likely to be merged first.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| Concept         | Explanation |\n",
    "|-----------------|-------------|\n",
    "| Scoring formula | `score = freq(pair) / (freq(A) * freq(B))` |\n",
    "| Purpose         | Prefer pairs that co-occur tightly but rarely appear alone |\n",
    "| Inspired by     | PMI (Pointwise Mutual Information) |\n",
    "| Benefit         | Builds meaningful subwords, avoids merging too-generic tokens |\n",
    "| Relation to probability | It's a **proxy for P(A, B) / P(A)P(B)** without full normalization |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a **code snippet** that computes WordPiece scores from a toy corpus or a **visual diagram** to illustrate this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba65d91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985d8d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "s1 = 'é'                  # U+00E9\n",
    "s2 = 'e\\u0301'            # U+0065 + U+0301\n",
    "\n",
    "print(s1 == s2)           # False (byte-wise different)\n",
    "\n",
    "# Normalize to NFC\n",
    "print(unicodedata.normalize('NFC', s1) == unicodedata.normalize('NFC', s2))  # ✅ True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7e360a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mﬁancé Héllò\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"ﬁancé Héllò\"\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_unicode_text(text):\n",
    "    # Normalize to NFKC: fixes ligatures, full-width chars, etc.\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    # Optional: remove combining accents (if needed)\n",
    "    # text = unicodedata.normalize(\"NFD\", text)\n",
    "    # text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "\n",
    "    # Replace non-breaking spaces and other weird whitespace\n",
    "    text = text.replace('\\u00A0', ' ')  # nbsp to normal space\n",
    "    text = re.sub(r'\\s+', ' ', text)   # collapse multiple spaces\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c8a97",
   "metadata": {},
   "source": [
    "Excellent observation — and yes, you're absolutely right to be curious about this. When parsing text from **PDFs**, you often see strange Unicode characters — ligatures, accents, non-breaking spaces, or weird punctuation. Let's unpack why this happens and what to do with it.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 Why PDF-Parsed Text Has Strange Unicode\n",
    "\n",
    "### PDFs are not meant to store clean text — they’re **visual layout formats**.  \n",
    "When you extract text from them, tools like `pdfplumber`, `PyMuPDF`, or `pdfminer` try to **reconstruct** the text from:\n",
    "- Low-level fonts\n",
    "- Encodings\n",
    "- Glyph mappings\n",
    "\n",
    "This can result in:\n",
    "\n",
    "| You See in PDF | Unicode Extracted | Explanation |\n",
    "|----------------|-------------------|-------------|\n",
    "| `ﬁ` | `U+FB01` (LATIN SMALL LIGATURE FI) | Ligature |\n",
    "| `‘` | `U+2018` (LEFT SINGLE QUOTATION MARK) | Smart quote |\n",
    "| ` ` | `U+00A0` (Non-breaking space) | Invisible whitespace |\n",
    "| `é` | May appear as `e + ́` (U+0065 + U+0301) | Decomposed |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What Happens if You Use This Raw Text Directly?\n",
    "\n",
    "If you feed this text directly into:\n",
    "- A tokenizer (like BERT’s WordPiece)\n",
    "- A search engine\n",
    "- A model trained on clean ASCII or Unicode NFC text\n",
    "\n",
    "👉 You **may get mismatched tokens**, unnecessary `[UNK]`s, or inconsistent embeddings.\n",
    "\n",
    "**Why?** Because:\n",
    "> `\"ﬁ\"` is not `\"f\"` + `\"i\"`  \n",
    "> `\" \"` is not a normal space  \n",
    "> `\"‘\"` is not a `'` apostrophe  \n",
    "> `\"e + ́\"` is not equal to `\"é\"` (precomposed)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ What Should You Do Before Using PDF Text?\n",
    "\n",
    "You should apply **Unicode normalization + basic cleaning** before using it in NLP pipelines.\n",
    "\n",
    "### Recommended Preprocessing Steps:\n",
    "\n",
    "```python\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_unicode_text(text):\n",
    "    # Normalize to NFKC: fixes ligatures, full-width chars, etc.\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    \n",
    "    # Optional: remove combining accents (if needed)\n",
    "    # text = unicodedata.normalize(\"NFD\", text)\n",
    "    # text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "\n",
    "    # Replace non-breaking spaces and other weird whitespace\n",
    "    text = text.replace('\\u00A0', ' ')  # nbsp to normal space\n",
    "    text = re.sub(r'\\s+', ' ', text)   # collapse multiple spaces\n",
    "\n",
    "    return text.strip()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Example:\n",
    "\n",
    "```python\n",
    "raw = \"ﬁancé\\u00A0Héllò\"\n",
    "print(clean_unicode_text(raw))  # Output: \"fiance hello\"\n",
    "```\n",
    "\n",
    "Now this cleaned text is safe for:\n",
    "- Tokenization\n",
    "- Indexing\n",
    "- Training / inference\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 TL;DR: If You're Using Text from PDFs...\n",
    "\n",
    "| Step | Why? |\n",
    "|------|------|\n",
    "| ✅ Unicode Normalization (`NFKC`) | Convert ligatures, full-width chars, smart quotes |\n",
    "| ✅ Normalize whitespace (`\\u00A0`) | Avoid layout-specific artifacts |\n",
    "| ❌ Don't use raw text | May break tokenizer, embeddings, search |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you're using a specific PDF parsing tool and I can help you **add normalization directly into your pipeline**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ebe1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb994b5",
   "metadata": {},
   "source": [
    "Unicode assigns a unique code point to each character (like an ID).\n",
    "UTF-8 maps that code point to a byte (or bytes) for storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b652a1f",
   "metadata": {},
   "source": [
    "🔥 Excellent real-world question — this is **exactly what backend developers face** when building file-upload APIs.\n",
    "\n",
    "You're asking:\n",
    "\n",
    "> 💭 If a user uploads a file via an API (which comes in as **binary**), should I:\n",
    "- Pass it around as **binary**?\n",
    "- Or **save it to disk**, then read and delete?\n",
    "\n",
    "Let’s break it down into **standard best practices** and real-world approaches 👇\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 How File Uploads Work in APIs (Concept)\n",
    "\n",
    "When a file is uploaded via an API:\n",
    "- It is sent as **binary data** (usually `multipart/form-data`)\n",
    "- In most Python web frameworks (like FastAPI, Flask, Django), you receive it as:\n",
    "  - A **file-like object** (stream)\n",
    "  - Or raw bytes (you can `.read()` it)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Standard API File Handling Options\n",
    "\n",
    "| Approach                          | Description |\n",
    "|----------------------------------|-------------|\n",
    "| 1️⃣ **In-memory handling**       | Use the file stream directly without saving |\n",
    "| 2️⃣ **Temporary file**           | Save to a temp file (e.g., in `/tmp/`), read, then delete |\n",
    "| 3️⃣ **Persistent save** (optional) | Save to a fixed location (e.g., uploads folder) if needed |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Best Practice: Use In-Memory (if file is small)\n",
    "\n",
    "### ⚡ FastAPI example:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/upload/\")\n",
    "async def upload_file(file: UploadFile = File(...)):\n",
    "    content = await file.read()  # 🔹 bytes in memory\n",
    "    text = content.decode('utf-8')  # decode if it's a text file\n",
    "    # process text or binary data here\n",
    "    return {\"filename\": file.filename, \"size\": len(content)}\n",
    "```\n",
    "\n",
    "✅ No need to save the file\n",
    "✅ Efficient for small to medium files (<10–50 MB)\n",
    "✅ Great for APIs that just process and return\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 If File is Large or Needs Disk Access\n",
    "\n",
    "Use a **temporary file**:\n",
    "\n",
    "```python\n",
    "import tempfile\n",
    "\n",
    "@app.post(\"/upload/\")\n",
    "async def upload_file(file: UploadFile = File(...)):\n",
    "    with tempfile.NamedTemporaryFile(delete=True) as temp:\n",
    "        temp.write(await file.read())\n",
    "        temp.seek(0)\n",
    "        data = temp.read()\n",
    "        # Process or forward this temp file\n",
    "```\n",
    "\n",
    "✅ Clean: file is auto-deleted  \n",
    "✅ Useful if:\n",
    "- You’re using 3rd-party tools that need file paths\n",
    "- Processing large files (e.g., PDFs, videos)\n",
    "\n",
    "---\n",
    "\n",
    "## ❓ Should You Always Create & Delete Files?\n",
    "\n",
    "| Situation                            | Recommendation |\n",
    "|--------------------------------------|----------------|\n",
    "| Just need to parse or read content   | ✅ In-memory (`file.read()` or `file.file`) |\n",
    "| Need to pass a file path to another lib | ✅ Temp file |\n",
    "| Need to store file permanently       | ✅ Save to disk or cloud |\n",
    "| Need to forward to another API       | ✅ Use file stream (no need to write to disk) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Bonus: Forwarding Upload to Another API\n",
    "\n",
    "You can forward the in-memory file without saving:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "@app.post(\"/proxy-upload/\")\n",
    "async def proxy_upload(file: UploadFile = File(...)):\n",
    "    files = {'file': (file.filename, await file.read(), file.content_type)}\n",
    "    response = requests.post(\"https://external-api/upload\", files=files)\n",
    "    return {\"status\": response.status_code}\n",
    "```\n",
    "\n",
    "✅ No need to save the file  \n",
    "✅ File passed directly as a stream to another API\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final Summary\n",
    "\n",
    "| Question | Answer |\n",
    "|---------|--------|\n",
    "| Should I always save uploaded files to disk? | ❌ No — use in-memory when possible |\n",
    "| When should I use a temp file? | If you need a file path or dealing with large files |\n",
    "| Can I pass uploaded binary directly to another API? | ✅ Yes — via in-memory bytes or file-like stream |\n",
    "| Do I need to decode uploaded binary? | ✅ Only if it's text-based (like `.txt`, `.csv`, `.json`) |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know your use case (e.g., uploading PDFs, text, images), and I can give you the cleanest FastAPI/Flask snippet for your flow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7016532",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
