{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5006f8f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧠 Additive Attention (Bahdanau Attention) \n",
    "\n",
    "- Developed to overcome the fixed-length context bottleneck in encoder-decoder RNN/LSTM models.\n",
    "- Instead of relying on a single vector for the entire input sequence, the decoder **attends to all encoder hidden states** while generating each output token.\n",
    "- For each output time step \\( i \\), the decoder computes a **context vector** \\( \\mathbf{c}_i \\) as a **weighted sum of encoder annotations** \\( \\mathbf{h}_j \\):\n",
    "\n",
    "  $$\n",
    "  \\mathbf{c}_i = \\sum_{j=1}^{T_x} \\alpha_{ij} \\mathbf{h}_j \\tag{5}\n",
    "  $$\n",
    "\n",
    "- The attention weights \\( \\alpha_{ij} \\) reflect the relevance of each encoder hidden state \\( \\mathbf{h}_j \\) to the current decoder state \\( \\mathbf{s}_{i-1} \\):\n",
    "\n",
    "  $$\n",
    "  \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})} \\tag{6}\n",
    "  $$\n",
    "\n",
    "- The **alignment score** \\( e_{ij} \\) is computed using a small feedforward neural network (the **alignment model**) that takes \\( \\mathbf{s}_{i-1} \\) and \\( \\mathbf{h}_j \\) as inputs:\n",
    "\n",
    "  $$\n",
    "  e_{ij} = \\mathbf{v}^T \\tanh(\\mathbf{W}_1 \\mathbf{s}_{i-1} + \\mathbf{W}_2 \\mathbf{h}_j + \\mathbf{b}) \\tag{7}\n",
    "  $$\n",
    "\n",
    "  - v: learnable weight vector\n",
    "  - W_1, W_2: learnable weight matrices\n",
    "  - b : bias term\n",
    "\n",
    "- This is called **additive attention** because the alignment model uses **addition** (not dot product) to combine the decoder and encoder hidden states.\n",
    "\n",
    "- The attention mechanism allows the decoder to dynamically **focus on relevant parts** of the input sequence during each generation step.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f11a1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ✅ In **Dot-Product Attention (Luong)** used in RNN-based encoder-decoder:\n",
    "\n",
    "- The **basic variant** computes the score as:\n",
    "  $$\n",
    "  e_{ij} = \\mathbf{s}_i^\\top \\mathbf{h}_j\n",
    "  $$\n",
    "  where:\n",
    "  - s_i: decoder hidden state at time step \\( i \\)\n",
    "  - h_j: encoder hidden state at time step \\( j \\)\n",
    "\n",
    "- This is **just a dot product** — **no learnable parameters** involved in this scoring step.\n",
    "\n",
    "---\n",
    "\n",
    "### Why use dot-product attention then?\n",
    "\n",
    "- ✅ **Efficiency**: It’s faster — no extra parameters or matrix ops.\n",
    "- ❌ **Less expressive**: Cannot learn complex alignments or transformations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4aaa9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔁 Limitation in RNN-based Attention Mechanisms (Both Additive & Dot-Product)\n",
    "\n",
    "- In both **additive attention (Bahdanau)** and **dot-product attention (Luong)**, attention is computed **from the decoder to the encoder**:\n",
    "  - The decoder focuses on different encoder hidden states while generating each output.\n",
    "  - But there is **no attention among encoder tokens themselves**.\n",
    "  \n",
    "- ⚠️ That means:\n",
    "  - **Input tokens do not attend to each other.**\n",
    "  - The encoder processes the input sequentially using RNNs (or LSTMs), and the final hidden states are used in attention.\n",
    "  - This limits the model’s ability to capture **long-range dependencies** and **global interactions** within the input sequence.\n",
    "\n",
    "- ✅ This limitation was addressed by the **Transformer model** (Vaswani et al., 2017), which introduced:\n",
    "  - **Self-attention**: each token in the input sequence **attends to all other tokens**, capturing global context.\n",
    "  - **Positional encoding**: to retain sequence order, since self-attention is not sequential like RNNs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd70ee7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔍 Why RNNs (even Bidirectional) Still Struggled to Capture Global Token Relations\n",
    "\n",
    "#### ✅ What Bidirectional RNNs *do*:\n",
    "- A **unidirectional RNN/LSTM** only sees the past: for time step \\( t \\), it processes from \\( x_1 \\) to \\( x_t \\).\n",
    "- A **bidirectional RNN (BiRNN)** adds another RNN that processes in reverse: from \\( x_T \\) to \\( x_1 \\).\n",
    "- So at each time step \\( t \\), the hidden state \\( h_t \\) contains information from both past and future tokens — i.e., **local context** from both directions.\n",
    "\n",
    "#### ❌ What they *still can't do well*:\n",
    "- Even with both directions, **each hidden state still represents only a compressed summary** of the context — and that compression is:\n",
    "  - **Sequential**\n",
    "  - **Fixed-size**\n",
    "  - **Hard to optimize over long sequences**\n",
    "- **No direct interaction** between distant tokens unless the information is passed **step-by-step**, which leads to:\n",
    "  - **Vanishing gradients**\n",
    "  - **Poor long-range dependency modeling**\n",
    "- For example, the relationship between \\( x_3 \\) and \\( x_{98} \\) has to be encoded **indirectly** through dozens of hidden states in between.\n",
    "\n",
    "---\n",
    "\n",
    "### 🤝 Why Attention Helped:\n",
    "- Attention allows the model to **look at all tokens at once**, not just rely on the final compressed hidden state.\n",
    "- But in **encoder-decoder attention**, the **input tokens (encoder hidden states) are never allowed to look at each other**.\n",
    "  - The decoder attends to the encoder outputs.\n",
    "  - But **encoder tokens don’t attend to other encoder tokens** — each hidden state is still generated sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Transformers: Game-Changer\n",
    "- Introduced **self-attention** in both encoder and decoder.\n",
    "- Now, **every token attends to every other token directly** (not through hidden states).\n",
    "- This enables the model to:\n",
    "  - Capture **global dependencies directly**\n",
    "  - Be **parallelizable** (no need for sequential processing)\n",
    "  - Better handle **long-range interactions**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 TL;DR:\n",
    "> Even bidirectional RNNs process sequences sequentially. They encode context *around* a token, but **don't model direct pairwise interactions** between all tokens. That’s why they can miss global relationships — especially over long distances. Attention fixed this for decoder → encoder, and Transformers fixed it fully with **self-attention**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28312c05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411e78d8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔁 Self-Attention & Contextualized Embeddings\n",
    "\n",
    "- **Self-attention** (or **intra-attention**) is a mechanism where:\n",
    "  - Each token in a sequence **attends to all other tokens**, including itself.\n",
    "  - It calculates **how much focus** to place on other tokens when updating its own representation.\n",
    "\n",
    "- The result is a **new embedding** for each token — called a **contextualized embedding**.\n",
    "\n",
    "- Unlike static embeddings (e.g., Word2Vec, GloVe), which assign **one fixed vector per word**, self-attention allows the **same word to have different embeddings depending on context**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Context Matters — The “Apple” Example\n",
    "\n",
    "Imagine these two sentences:\n",
    "\n",
    "1. **\"I ate a juicy apple after lunch.\"** 🍎  \n",
    "2. **\"I updated my Apple phone to the latest version.\"** 📱\n",
    "\n",
    "- In both cases, the word **\"apple\"** is spelled the same.\n",
    "- But their **meanings are completely different** — fruit vs tech company.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ How Self-Attention Helps:\n",
    "\n",
    "| Sentence                              | Context Around \"Apple\"         | Resulting Embedding          |\n",
    "|---------------------------------------|--------------------------------|------------------------------|\n",
    "| \"I ate a juicy **apple** after lunch\" | Tokens like *ate*, *juicy*, *lunch* | Embedding leans toward **fruit** 🍏 |\n",
    "| \"I updated my **Apple** phone...\"     | Tokens like *updated*, *phone*, *version* | Embedding leans toward **technology** 📱 |\n",
    "\n",
    "- Thanks to **self-attention**, \"apple\" attends to words around it.\n",
    "- The model **learns the meaning from context**, so the embedding for \"apple\" becomes **context-aware**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Key Benefits of Contextualized Embeddings:\n",
    "\n",
    "- 🔍 **Disambiguation**: Helps models distinguish between multiple meanings of the same word.\n",
    "- 🧠 **Rich understanding**: Better grasp of semantics and syntax.\n",
    "- 💬 **Improved performance**: Boosts accuracy in downstream tasks like translation, sentiment analysis, QA, etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3a3e4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ Multi-Head Self-Attention\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "- **Embedding dimension** = 512  \n",
    "- **Number of heads** = 8  \n",
    "- Therefore, each head operates on **512 / 8 = 64** dimensions  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔢 Step-by-Step Breakdown\n",
    "\n",
    "#### 1. **Start with token input:**\n",
    "- You begin with the **input embedding**:\n",
    "  $$\n",
    "  \\text{input} = \\text{token embedding} + \\text{positional encoding} \\in \\mathbb{R}^{512}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Linear projections per head:**\n",
    "- We **project it into separate Q, K, V vectors for each head** using **separate weight matrices** for each head.\n",
    "  \n",
    "- So for **each head \\( i \\in [1, 8] \\)**:\n",
    "  $$\n",
    "  Q^{(i)} = X W_Q^{(i)}, \\quad K^{(i)} = X W_K^{(i)}, \\quad V^{(i)} = X W_V^{(i)}\n",
    "  $$\n",
    "  \n",
    "  where:\n",
    "  -  $$W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \\in \\mathbb{R}^{512 \\times 64}$$ \n",
    "  -  $$Q^{(i)}, K^{(i)}, V^{(i)} \\in \\mathbb{R}^{n \\times 64}$$ \n",
    "   for sequence length n \n",
    "\n",
    "✅ So yes: each matrix maps the **full 512-dim input** to a **64-dim subspace** for that head.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Self-attention for each head:**\n",
    "- Each head computes scaled dot-product attention **independently**:\n",
    "  $$\n",
    "  \\text{Attention}^{(i)} = \\text{softmax} \\left( \\frac{Q^{(i)} {K^{(i)}}^\\top}{\\sqrt{64}} \\right) V^{(i)}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Concatenation of heads:**\n",
    "- After computing attention outputs for all 8 heads (each of size \\( \\mathbb{R}^{n \\times 64} \\)), you **concatenate them** along the feature dimension:\n",
    "  $$\n",
    "  \\text{Concat} = [\\text{head}_1; \\text{head}_2; \\dots; \\text{head}_8] \\in \\mathbb{R}^{n \\times 512}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Final linear projection (W₀):**\n",
    "- You apply a final learned linear layer:\n",
    "  $$\n",
    "  \\text{Output} = \\text{Concat} \\cdot W_O\n",
    "  $$\n",
    "  where \\( W_O \\in \\mathbb{R}^{512 \\times 512} \\)\n",
    "\n",
    "- This projects the concatenated output **back to the original embedding dimension (512)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4466590",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧠 Time Complexity of Self-Attention\n",
    "\n",
    "Let:\n",
    "- \\( n \\): sequence length\n",
    "- \\( d \\): embedding dimension\n",
    "- \\( h \\): number of heads\n",
    "- \\( d_k = d/h \\): dimension per head (usually, \\( d_k \\approx 64 \\))\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 1. **Self-Attention (Single Head)**\n",
    "\n",
    "Self-attention requires the following operations per layer:\n",
    "\n",
    "#### 📌 Step-by-step breakdown:\n",
    "\n",
    "| Operation | Complexity |\n",
    "|----------|------------|\n",
    "| **Linear projections** (Q, K, V from input \\( X \\in \\mathbb{R}^{n \\times d} \\)) | \\( O(n \\cdot d^2) \\) |\n",
    "| **Dot product attention (QKᵀ)** | \\( O(n^2 \\cdot d) \\) |\n",
    "| **Softmax over \\( n \\) tokens** | \\( O(n^2) \\) |\n",
    "| **Multiply attention weights with V** | \\( O(n^2 \\cdot d) \\) |\n",
    "\n",
    "✅ **Total (Single-Head)**:  \n",
    "$$\n",
    "\\boxed{O(n^2 \\cdot d + n \\cdot d^2)}\n",
    "$$\n",
    "\n",
    "- \\( O(n^2 \\cdot d) \\): due to pairwise interactions for attention\n",
    "- \\( O(n \\cdot d^2) \\): due to the linear projections (Q, K, V)\n",
    "\n",
    "> The dominating term is usually **\\( O(n^2 \\cdot d) \\)** due to the attention matrix \\( QK^\\top \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔀 2. **Multi-Head Attention**\n",
    "\n",
    "In multi-head attention, we perform the attention operation **in parallel for each head**, with smaller dimensions \\( d_k = d/h \\), and then concatenate.\n",
    "\n",
    "So:\n",
    "- **Per head**: \\( O(n^2 \\cdot d_k + n \\cdot d \\cdot d_k) \\)\n",
    "- Across **\\( h \\)** heads: \\( h \\cdot O(n^2 \\cdot d_k) = O(n^2 \\cdot d) \\)\n",
    "\n",
    "Final projection (concatenated heads back to \\( d \\)-dim):\n",
    "- \\( O(n \\cdot d^2) \\)\n",
    "\n",
    "✅ **Total (Multi-Head)**:  \n",
    "$$\n",
    "\\boxed{O(n^2 \\cdot d + n \\cdot d^2)}\n",
    "$$\n",
    "\n",
    "> Same as single-head in big-O — but **more efficient in practice due to parallelism**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Bottleneck Insight\n",
    "\n",
    "- **Quadratic time complexity in sequence length \\( n \\)**:  \n",
    "  - \\( O(n^2 \\cdot d) \\) → the reason why Transformers become slow for long sequences (e.g. long documents or audio).\n",
    "  - This comes from computing \\( QK^\\top \\in \\mathbb{R}^{n \\times n} \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Summary Table\n",
    "\n",
    "| Mechanism          | Time Complexity               | Notes                        |\n",
    "|--------------------|-------------------------------|------------------------------|\n",
    "| Self-Attention     | \\( O(n^2 \\cdot d + n \\cdot d^2) \\) | Quadratic in \\( n \\), due to token-token interactions |\n",
    "| Multi-Head Attention | \\( O(n^2 \\cdot d + n \\cdot d^2) \\) | Same complexity, but more powerful with multiple subspaces |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a visual to show how this scales or want a comparison with **Recurrent** or **Convolutional** layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab54ef2",
   "metadata": {},
   "source": [
    "## What is the need of scaling with the 1/sqrt(d_k) why didn't we use some other number instead of d_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173d98a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
